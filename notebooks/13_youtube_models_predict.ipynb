{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f1a8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\notebooks\n",
      "PROJECT_ROOT: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\n",
      "MODEL_DIR: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\n",
      "PATH_TRENDING_DAILY: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\processed\\01_daily_accumulated\\youtube_trending_videos_daily_kr_v1.csv\n",
      "PATH_CHANNEL_DAILY : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\processed\\01_daily_accumulated\\youtube_channels_daily_stats_kr_v1.csv\n",
      "PATH_COMMENTS_RAW  : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\processed\\01_daily_accumulated\\youtube_comments_raw_kr_v1.csv\n",
      "PATH_COMMENT_VIDEO_FEATS: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\processed\\02_comment_features\\youtube_comment_features_video_level_kr_v1.csv\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", 200)\n",
    "np.random.seed(42)\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    p = Path.cwd()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \"data\").exists() and (parent / \"notebooks\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "def safe_read_csv(path: Path | None):\n",
    "    \"\"\"파일이 없거나 path가 None이면 None 반환\"\"\"\n",
    "    if path is None:\n",
    "        return None\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(\"⚠️ not found:\", path)\n",
    "        return None\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def latest_versioned_csv(folder: Path, base_name: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    folder 안에서 base_name_v{n}.csv 중 가장 큰 n 파일 Path 반환\n",
    "    없으면 None\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+)\\.csv$\")\n",
    "    best_v, best_path = None, None\n",
    "\n",
    "    for f in folder.glob(f\"{base_name}_v*.csv\"):\n",
    "        m = pattern.match(f.name)\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v, best_path = v, f\n",
    "\n",
    "    return best_path\n",
    "\n",
    "def next_versioned_file(folder: Path, base_name: str, ext: str = \".csv\") -> Path:\n",
    "    \"\"\"\n",
    "    folder 안에서 base_name_v{n}{ext} 다음 버전 경로 반환 (파일 저장용)\n",
    "    \"\"\"\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "\n",
    "    versions = []\n",
    "    for f in folder.glob(f\"{base_name}_v*{ext}\"):\n",
    "        m = pattern.match(f.name)\n",
    "        if m:\n",
    "            versions.append(int(m.group(1)))\n",
    "\n",
    "    v = (max(versions) + 1) if versions else 1\n",
    "    return folder / f\"{base_name}_v{v}{ext}\"\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "# ✅ 모델 폴더\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n",
    "\n",
    "# ✅ 13번은 API 누적 데이터만 사용 (사용자 조건 반영)\n",
    "ACCUM_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"01_daily_accumulated\"\n",
    "FEAT_DIR  = PROJECT_ROOT / \"data\" / \"processed\" / \"02_comment_features\"\n",
    "\n",
    "PATH_TRENDING_DAILY = ACCUM_DIR / \"youtube_trending_videos_daily_kr_v1.csv\"\n",
    "PATH_CHANNEL_DAILY  = ACCUM_DIR / \"youtube_channels_daily_stats_kr_v1.csv\"\n",
    "PATH_COMMENTS_RAW   = ACCUM_DIR / \"youtube_comments_raw_kr_v1.csv\"\n",
    "\n",
    "# ✅ 댓글 기반 비디오 레벨 feature (04번 산출물)\n",
    "PATH_COMMENT_VIDEO_FEATS = FEAT_DIR / \"youtube_comment_features_video_level_kr_v1.csv\"\n",
    "\n",
    "# ✅ 기존 코드 호환(중요): build_trending_duration_dataset()에서 쓰는 이름을 맞춰줌\n",
    "PATH_VCF_BY_VIDEO = PATH_COMMENT_VIDEO_FEATS\n",
    "PATH_VCF = None  # 두 번째 merge는 스킵되도록\n",
    "\n",
    "print(\"PATH_TRENDING_DAILY:\", PATH_TRENDING_DAILY)\n",
    "print(\"PATH_CHANNEL_DAILY :\", PATH_CHANNEL_DAILY)\n",
    "print(\"PATH_COMMENTS_RAW  :\", PATH_COMMENTS_RAW)\n",
    "print(\"PATH_COMMENT_VIDEO_FEATS:\", PATH_COMMENT_VIDEO_FEATS)\n",
    "\n",
    "# 존재 체크\n",
    "for p in [PATH_TRENDING_DAILY, PATH_CHANNEL_DAILY, PATH_COMMENTS_RAW]:\n",
    "    if not p.exists():\n",
    "        print(\"⚠️ not found:\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61427f96",
   "metadata": {},
   "source": [
    "## 1) 영상 트렌딩 유지기간 예측 (video_id 단위 회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab0fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video dataset: (51, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_duration_days</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>region</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>first_trending_date</th>\n",
       "      <th>views_day1</th>\n",
       "      <th>likes_day1</th>\n",
       "      <th>comments_day1</th>\n",
       "      <th>views_mean</th>\n",
       "      <th>views_max</th>\n",
       "      <th>likes_mean</th>\n",
       "      <th>likes_max</th>\n",
       "      <th>comments_mean</th>\n",
       "      <th>comments_max</th>\n",
       "      <th>days_to_first_trending</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>unique_authors</th>\n",
       "      <th>mean_like_count</th>\n",
       "      <th>mean_text_len</th>\n",
       "      <th>url_ratio</th>\n",
       "      <th>hashtag_ratio</th>\n",
       "      <th>mention_ratio</th>\n",
       "      <th>korean_comment_ratio</th>\n",
       "      <th>mean_hangul_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-WGFbInX6JI</td>\n",
       "      <td>1</td>\n",
       "      <td>UC1q4Ihlv_YhLELw-ijE0Diw</td>\n",
       "      <td>KR</td>\n",
       "      <td>20</td>\n",
       "      <td>2026-01-30 12:30:00</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>89876</td>\n",
       "      <td>2820</td>\n",
       "      <td>379</td>\n",
       "      <td>89876.0</td>\n",
       "      <td>89876</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>2820</td>\n",
       "      <td>379.0</td>\n",
       "      <td>379</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>181.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>3.944751</td>\n",
       "      <td>36.679558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.730608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0HXwT4gefnQ</td>\n",
       "      <td>1</td>\n",
       "      <td>UCpqyr6h4RCXCEswHlkSjykA</td>\n",
       "      <td>KR</td>\n",
       "      <td>20</td>\n",
       "      <td>2026-01-30 09:00:02</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>296006</td>\n",
       "      <td>16525</td>\n",
       "      <td>1296</td>\n",
       "      <td>296006.0</td>\n",
       "      <td>296006</td>\n",
       "      <td>16525.0</td>\n",
       "      <td>16525</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>1296</td>\n",
       "      <td>0.624977</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>25.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0ZrO8_AKVcc</td>\n",
       "      <td>1</td>\n",
       "      <td>UC28EaHd6V5EFqgaKZG33pIQ</td>\n",
       "      <td>KR</td>\n",
       "      <td>10</td>\n",
       "      <td>2026-01-27 09:00:01</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>106814</td>\n",
       "      <td>1571</td>\n",
       "      <td>62</td>\n",
       "      <td>106814.0</td>\n",
       "      <td>106814</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>1571</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62</td>\n",
       "      <td>3.624988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  trending_duration_days                channel_id region  \\\n",
       "0  -WGFbInX6JI                       1  UC1q4Ihlv_YhLELw-ijE0Diw     KR   \n",
       "1  0HXwT4gefnQ                       1  UCpqyr6h4RCXCEswHlkSjykA     KR   \n",
       "2  0ZrO8_AKVcc                       1  UC28EaHd6V5EFqgaKZG33pIQ     KR   \n",
       "\n",
       "   category_id        publish_date first_trending_date  views_day1  \\\n",
       "0           20 2026-01-30 12:30:00          2026-01-31       89876   \n",
       "1           20 2026-01-30 09:00:02          2026-01-31      296006   \n",
       "2           10 2026-01-27 09:00:01          2026-01-31      106814   \n",
       "\n",
       "   likes_day1  comments_day1  views_mean  views_max  likes_mean  likes_max  \\\n",
       "0        2820            379     89876.0      89876      2820.0       2820   \n",
       "1       16525           1296    296006.0     296006     16525.0      16525   \n",
       "2        1571             62    106814.0     106814      1571.0       1571   \n",
       "\n",
       "   comments_mean  comments_max  days_to_first_trending  comment_count  \\\n",
       "0          379.0           379                0.479167          181.0   \n",
       "1         1296.0          1296                0.624977           52.0   \n",
       "2           62.0            62                3.624988            NaN   \n",
       "\n",
       "   unique_authors  mean_like_count  mean_text_len  url_ratio  hashtag_ratio  \\\n",
       "0           169.0         3.944751      36.679558        0.0            0.0   \n",
       "1            52.0         0.096154      25.230769        0.0            0.0   \n",
       "2             NaN              NaN            NaN        NaN            NaN   \n",
       "\n",
       "   mention_ratio  korean_comment_ratio  mean_hangul_ratio  \n",
       "0            0.0                   1.0           0.730608  \n",
       "1            0.0                   1.0           0.829411  \n",
       "2            NaN                   NaN                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_trending_duration_dataset():\n",
    "    trending = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "\n",
    "    # ✅ merge 안정성: video_id 타입 통일\n",
    "    trending[\"video_id\"] = trending[\"video_id\"].astype(str)\n",
    "\n",
    "    # ✅ 호환: 예전 누적 파일이 collected_date를 썼다면 date로 변환\n",
    "    if \"date\" not in trending.columns and \"collected_date\" in trending.columns:\n",
    "        trending = trending.rename(columns={\"collected_date\": \"date\"})\n",
    "\n",
    "    # 날짜 tz 통일(UTC로 파싱 후 tz 제거 -> naive)\n",
    "    trending[\"date\"] = pd.to_datetime(trending[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    if \"publish_date\" in trending.columns:\n",
    "        trending[\"publish_date\"] = pd.to_datetime(trending[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # target: video_id별 트렌딩 유지기간(일)\n",
    "    y = (trending.groupby(\"video_id\")[\"date\"]\n",
    "                .nunique()\n",
    "                .rename(\"trending_duration_days\")\n",
    "                .reset_index())\n",
    "\n",
    "    # first day snapshot\n",
    "    first_day = (trending.sort_values([\"video_id\", \"date\"])\n",
    "                         .groupby(\"video_id\", as_index=False)\n",
    "                         .first())\n",
    "\n",
    "    cols = [\"video_id\"]\n",
    "    for c in [\"channel_id\", \"region\", \"category_id\", \"publish_date\", \"date\", \"views\", \"likes\", \"comments\"]:\n",
    "        if c in first_day.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    first_day = first_day[cols].rename(columns={\n",
    "        \"views\": \"views_day1\",\n",
    "        \"likes\": \"likes_day1\",\n",
    "        \"comments\": \"comments_day1\",\n",
    "        \"date\": \"first_trending_date\"\n",
    "    })\n",
    "\n",
    "    # aggregates\n",
    "    agg = (trending.groupby(\"video_id\")\n",
    "                  .agg(\n",
    "                      views_mean=(\"views\", \"mean\"),\n",
    "                      views_max=(\"views\", \"max\"),\n",
    "                      likes_mean=(\"likes\", \"mean\"),\n",
    "                      likes_max=(\"likes\", \"max\"),\n",
    "                      comments_mean=(\"comments\", \"mean\"),\n",
    "                      comments_max=(\"comments\", \"max\"),\n",
    "                      trending_days=(\"date\", \"nunique\"),\n",
    "                  )\n",
    "                  .reset_index())\n",
    "\n",
    "    df = y.merge(first_day, on=\"video_id\", how=\"left\").merge(agg, on=\"video_id\", how=\"left\")\n",
    "\n",
    "    # merge 이후 dtype 꼬임 방지: 다시 datetime 강제\n",
    "    if \"publish_date\" in df.columns:\n",
    "        df[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    if \"first_trending_date\" in df.columns:\n",
    "        df[\"first_trending_date\"] = pd.to_datetime(df[\"first_trending_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # days_to_first_trending\n",
    "    df[\"days_to_first_trending\"] = (\n",
    "        (df[\"first_trending_date\"] - df[\"publish_date\"]).dt.total_seconds() / 86400.0\n",
    "        if (\"publish_date\" in df.columns and \"first_trending_date\" in df.columns)\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    # merge comment features (video-level)\n",
    "    vcfv = safe_read_csv(PATH_VCF_BY_VIDEO)\n",
    "    if vcfv is not None and \"video_id\" in vcfv.columns:\n",
    "        vcfv = vcfv.copy()\n",
    "        vcfv[\"video_id\"] = vcfv[\"video_id\"].astype(str)\n",
    "        vcfv = vcfv[~vcfv[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcfv = vcfv.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcfv.drop(columns=[\"category_name\"], errors=\"ignore\"), on=\"video_id\", how=\"left\")\n",
    "\n",
    "    vcf = safe_read_csv(PATH_VCF)\n",
    "    if vcf is not None and \"video_id\" in vcf.columns:\n",
    "        vcf = vcf.copy()\n",
    "        vcf[\"video_id\"] = vcf[\"video_id\"].astype(str)\n",
    "        vcf = vcf[~vcf[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcf = vcf.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcf, on=\"video_id\", how=\"left\", suffixes=(\"\", \"_vcf\"))\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # leakage 제거\n",
    "    df.drop(columns=[\"trending_days\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # ✅ raw/ID/원문 컬럼 제거(혹시 섞여 들어온 경우)\n",
    "    DROP_COLS = [\"comment_id\", \"comment_publishedAt\", \"text\", \"run_id\", \"category_name\", \"country\", \"likeCount\"]\n",
    "    df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_video = build_trending_duration_dataset()\n",
    "print(\"video dataset:\", df_video.shape)\n",
    "display(df_video.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff229a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trending_duration_model(df, test_size=0.2, random_state=42, prefix=\"trending_duration\"):\n",
    "    import os\n",
    "\n",
    "    target_col = \"trending_duration_days\"\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    print(\"DEBUG MODEL_DIR =\", MODEL_DIR)\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    X = df.drop(columns=[\"video_id\", target_col], errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # 1) datetime 컬럼 -> 숫자화(ts/dow)\n",
    "    datetime_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X.columns and c not in datetime_cols:\n",
    "            X[c] = pd.to_datetime(X[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            datetime_cols.append(c)\n",
    "\n",
    "    datetime_cols = list(dict.fromkeys(datetime_cols))\n",
    "    for c in datetime_cols:\n",
    "        dt = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "        ts = dt.astype(\"int64\")\n",
    "        ts = pd.Series(ts, index=X.index).where(dt.notna(), np.nan) / 1e9\n",
    "        X[c + \"_ts\"] = ts\n",
    "        X[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    if datetime_cols:\n",
    "        X = X.drop(columns=datetime_cols, errors=\"ignore\")\n",
    "\n",
    "    # 2) object지만 숫자열이면 numeric 변환\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 3) 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 4) 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        print(\"Drop constant cols:\", const_cols[:20], \"...\" if len(const_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    # 5) cat/num 분리\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    # 6) 전처리 파이프라인\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    # 7) split & fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"DEBUG: fit start | X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(\"DEBUG: fit done\")\n",
    "\n",
    "    pred = pipe.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"[Trending Duration] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "    print(\"Features used:\", X.shape[1])\n",
    "\n",
    "    # ✅ v1/v2 자동 저장 + 저장 확인\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "\n",
    "    print(\"DEBUG: about to save\")\n",
    "    print(\"DEBUG model_path =\", model_path)\n",
    "    print(\"DEBUG cols_path  =\", cols_path)\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    assert model_path.exists(), f\"모델 저장 실패: {model_path}\"\n",
    "    assert cols_path.exists(),  f\"컬럼 저장 실패: {cols_path}\"\n",
    "\n",
    "\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a944c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN TRAIN START\n",
      "DEBUG MODEL_DIR = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\n",
      "Drop constant cols: ['region', 'korean_comment_ratio', 'first_trending_date_ts', 'first_trending_date_dow'] \n",
      "DEBUG: fit start | X_train: (40, 22) X_test: (11, 22)\n",
      "DEBUG: fit done\n",
      "[Trending Duration] MAE: 0.0000  RMSE: 0.0000  R2: 1.0000\n",
      "Features used: 22\n",
      "DEBUG: about to save\n",
      "DEBUG model_path = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n",
      "DEBUG cols_path  = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n",
      "saved:\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n",
      "RUN TRAIN DONE\n",
      "RETURNED: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"RUN TRAIN START\")\n",
    "video_model, video_feature_cols, video_model_path, video_cols_path = train_trending_duration_model(df_video)\n",
    "print(\"RUN TRAIN DONE\")\n",
    "print(\"RETURNED:\", video_model_path, video_cols_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2233953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template columns: 22\n",
      "loaded from: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['channel_id',\n",
       " 'category_id',\n",
       " 'views_day1',\n",
       " 'likes_day1',\n",
       " 'comments_day1',\n",
       " 'views_mean',\n",
       " 'views_max',\n",
       " 'likes_mean',\n",
       " 'likes_max',\n",
       " 'comments_mean',\n",
       " 'comments_max',\n",
       " 'days_to_first_trending',\n",
       " 'comment_count',\n",
       " 'unique_authors',\n",
       " 'mean_like_count',\n",
       " 'mean_text_len',\n",
       " 'url_ratio',\n",
       " 'hashtag_ratio',\n",
       " 'mention_ratio',\n",
       " 'mean_hangul_ratio',\n",
       " 'publish_date_ts',\n",
       " 'publish_date_dow']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _latest_versioned_file(base_dir: str, base_name: str, ext: str = \".joblib\") -> str:\n",
    "    \"\"\"base_name_vN.joblib 중 가장 큰 N을 반환\"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "    best_v = None\n",
    "    best_path = None\n",
    "    for f in os.listdir(base_dir):\n",
    "        m = pattern.match(f)\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v = v\n",
    "                best_path = os.path.join(base_dir, f)\n",
    "    if best_path is None:\n",
    "        raise FileNotFoundError(f\"{base_dir} 에 '{base_name}_v*.joblib' 파일이 없습니다.\")\n",
    "    return best_path\n",
    "\n",
    "\n",
    "def _load_latest_model_and_cols(model_base_name: str, cols_base_name: str):\n",
    "    model_path = _latest_versioned_file(MODEL_DIR, model_base_name)\n",
    "    cols_path  = _latest_versioned_file(MODEL_DIR, cols_base_name)\n",
    "    pipe = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(cols_path)\n",
    "    return pipe, feature_cols, model_path, cols_path\n",
    "\n",
    "\n",
    "def predict_trending_duration(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # (선택) 날짜 입력을 줬다면 학습 때와 동일하게 파생 생성\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X_new.columns:\n",
    "            dt = pd.to_datetime(X_new[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            X_new[c + \"_ts\"] = dt.astype(\"int64\") / 1e9\n",
    "            X_new[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    # 학습 피처 컬럼에 맞추기(없는 컬럼은 NaN)\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "    X_new = X_new[feature_cols]\n",
    "\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "    return float(pred)\n",
    "\n",
    "\n",
    "def trending_duration_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "\n",
    "tpl = trending_duration_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"trending_duration_model\"))\n",
    "list(tpl.keys())[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred trending duration(days): 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_video_input = {\n",
    "    \"region\": \"KR\",\n",
    "    \"category_id\": 24,\n",
    "    \"views_day1\": 120000,\n",
    "    \"likes_day1\": 8000,\n",
    "    \"comments_day1\": 1500,\n",
    "    \"views_mean\": 180000,\n",
    "    \"views_max\": 350000,\n",
    "    \"likes_mean\": 12000,\n",
    "    \"likes_max\": 22000,\n",
    "    \"comments_mean\": 2000,\n",
    "    \"comments_max\": 4200,\n",
    "    \"days_to_first_trending\": 2.0,\n",
    "}\n",
    "print(\"pred trending duration(days):\", predict_trending_duration(example_video_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfede4a",
   "metadata": {},
   "source": [
    "## 2) 채널 성장 예측 (channel_id×date 단위 회귀)\n",
    "타깃: subscriber_growth_h = (h일 뒤 구독자수 - 오늘 구독자수). 기본 h=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19915d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel dataset: (0, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>run_ts_utc</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>created_date</th>\n",
       "      <th>subscriber_count</th>\n",
       "      <th>views_total</th>\n",
       "      <th>video_count_total</th>\n",
       "      <th>country</th>\n",
       "      <th>subscriber_future</th>\n",
       "      <th>subscriber_growth_h</th>\n",
       "      <th>subs_delta_1d</th>\n",
       "      <th>views_delta_1d</th>\n",
       "      <th>video_count_delta_1d</th>\n",
       "      <th>subs_delta_7d_mean</th>\n",
       "      <th>views_delta_7d_mean</th>\n",
       "      <th>trending_video_cnt</th>\n",
       "      <th>trending_views_sum</th>\n",
       "      <th>trending_likes_sum</th>\n",
       "      <th>trending_comments_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, run_ts_utc, channel_id, channel_name, created_date, subscriber_count, views_total, video_count_total, country, subscriber_future, subscriber_growth_h, subs_delta_1d, views_delta_1d, video_count_delta_1d, subs_delta_7d_mean, views_delta_7d_mean, trending_video_cnt, trending_views_sum, trending_likes_sum, trending_comments_sum]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_channel_growth_dataset(horizon_days=7):\n",
    "    ch = pd.read_csv(PATH_CHANNEL_DAILY)\n",
    "\n",
    "    # 날짜 파싱\n",
    "    ch[\"date\"] = pd.to_datetime(ch.get(\"date\"), errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    ch = ch.sort_values([\"channel_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # 숫자형 강제\n",
    "    for c in [\"subscriber_count\", \"views_total\", \"video_count_total\"]:\n",
    "        if c in ch.columns:\n",
    "            ch[c] = pd.to_numeric(ch[c], errors=\"coerce\")\n",
    "\n",
    "    # 타깃: h일 뒤 구독자 증가량\n",
    "    ch[\"subscriber_future\"] = ch.groupby(\"channel_id\")[\"subscriber_count\"].shift(-horizon_days)\n",
    "    ch[\"subscriber_growth_h\"] = ch[\"subscriber_future\"] - ch[\"subscriber_count\"]\n",
    "\n",
    "    # 변화량 피처\n",
    "    ch[\"subs_delta_1d\"] = ch.groupby(\"channel_id\")[\"subscriber_count\"].diff(1)\n",
    "    ch[\"subs_delta_1d\"] = ch[\"subs_delta_1d\"].fillna(0)\n",
    "\n",
    "    if \"views_total\" in ch.columns:\n",
    "        ch[\"views_delta_1d\"] = ch.groupby(\"channel_id\")[\"views_total\"].diff(1).fillna(0)\n",
    "\n",
    "    if \"video_count_total\" in ch.columns:\n",
    "        ch[\"video_count_delta_1d\"] = ch.groupby(\"channel_id\")[\"video_count_total\"].diff(1).fillna(0)\n",
    "\n",
    "    # 7일 평균 추세(초기 NaN 최소화)\n",
    "    ch[\"subs_delta_7d_mean\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subs_delta_1d\"]\n",
    "          .rolling(7, min_periods=1)\n",
    "          .mean()\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    if \"views_delta_1d\" in ch.columns:\n",
    "        ch[\"views_delta_7d_mean\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"views_delta_1d\"]\n",
    "              .rolling(7, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # 트렌딩(채널-일자 집계) merge\n",
    "    tr = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "    tr[\"date\"] = pd.to_datetime(tr.get(\"date\"), errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    if \"channel_id\" in tr.columns:\n",
    "        tr[\"channel_id\"] = tr[\"channel_id\"].astype(str)\n",
    "    if \"video_id\" in tr.columns:\n",
    "        tr[\"video_id\"] = tr[\"video_id\"].astype(str)\n",
    "\n",
    "    # 기본 집계\n",
    "    agg_dict = {\"video_id\": \"nunique\"}\n",
    "    for c in [\"views\", \"likes\", \"comments\"]:\n",
    "        if c in tr.columns:\n",
    "            tr[c] = pd.to_numeric(tr[c], errors=\"coerce\")\n",
    "            agg_dict[c] = \"sum\"\n",
    "\n",
    "    tr_agg = (tr.groupby([\"channel_id\", \"date\"], as_index=False)\n",
    "                .agg(agg_dict)\n",
    "                .rename(columns={\n",
    "                    \"video_id\": \"trending_video_cnt\",\n",
    "                    \"views\": \"trending_views_sum\",\n",
    "                    \"likes\": \"trending_likes_sum\",\n",
    "                    \"comments\": \"trending_comments_sum\",\n",
    "                }))\n",
    "\n",
    "    df = ch.merge(tr_agg, on=[\"channel_id\", \"date\"], how=\"left\")\n",
    "\n",
    "    # 트렌딩 관련 결측은 0 처리 (그 날 트렌딩 영향 없음)\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"trending_\"):\n",
    "            df[c] = df[c].fillna(0)\n",
    "\n",
    "    # inf 처리\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 타깃 NaN (끝부분 h일치) 제거\n",
    "    df = df[df[\"subscriber_growth_h\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_channel = build_channel_growth_dataset(horizon_days=7)\n",
    "print(\"channel dataset:\", df_channel.shape)\n",
    "display(df_channel.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f81e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop all-NaN cols: ['run_ts_utc', 'channel_name', 'created_date', 'subscriber_count', 'views_total', 'video_count_total', 'country', 'subs_delta_1d', 'views_delta_1d', 'video_count_delta_1d', 'subs_delta_7d_mean', 'views_delta_7d_mean', 'trending_video_cnt', 'trending_views_sum', 'trending_likes_sum', 'trending_comments_sum', 'dayofweek'] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m -\u001b[39m\u001b[33m\"\u001b[39m, cols_path)\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pipe, \u001b[38;5;28mlist\u001b[39m(X.columns), model_path, cols_path\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m channel_model, channel_feature_cols, channel_model_path, channel_cols_path = \u001b[43mtrain_channel_growth_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_channel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mlen\u001b[39m(channel_feature_cols), channel_feature_cols[:\u001b[32m25\u001b[39m], channel_model_path, channel_cols_path\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_channel_growth_model\u001b[39m\u001b[34m(df, test_size, random_state, prefix)\u001b[39m\n\u001b[32m     63\u001b[39m model = RandomForestRegressor(\n\u001b[32m     64\u001b[39m     n_estimators=\u001b[32m900\u001b[39m, random_state=random_state, n_jobs=-\u001b[32m1\u001b[39m, min_samples_leaf=\u001b[32m2\u001b[39m\n\u001b[32m     65\u001b[39m )\n\u001b[32m     67\u001b[39m pipe = Pipeline([(\u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, pre), (\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, model)])\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m pipe.fit(X_train, y_train)\n\u001b[32m     74\u001b[39m pred = pipe.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\73bib\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\73bib\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2919\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\73bib\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2499\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2496\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2503\u001b[39m     )\n\u001b[32m   2505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "def train_channel_growth_model(df, test_size=0.2, random_state=42, prefix=\"channel_growth\"):\n",
    "    target_col = \"subscriber_growth_h\"\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "\n",
    "    X = df.drop(columns=[target_col, \"subscriber_future\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # ID 제거(식별자 과적합 방지)\n",
    "    X = X.drop(columns=[\"channel_id\"], errors=\"ignore\")\n",
    "\n",
    "    # date -> dayofweek\n",
    "    if \"date\" in X.columns:\n",
    "        X[\"date\"] = pd.to_datetime(X[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X[\"dayofweek\"] = X[\"date\"].dt.dayofweek\n",
    "        X = X.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    # inf 제거\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # object인데 숫자열이면 numeric으로 바꾸기(보수적)\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=900, random_state=random_state, n_jobs=-1, min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"[Channel Growth] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "    print(\"Features used:\", X.shape[1])\n",
    "\n",
    "    # ✅ v1/v2 자동 저장 (프로젝트 루트/models)\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path\n",
    "\n",
    "\n",
    "channel_model, channel_feature_cols, channel_model_path, channel_cols_path = train_channel_growth_model(df_channel)\n",
    "len(channel_feature_cols), channel_feature_cols[:25], channel_model_path, channel_cols_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90faca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_channel_growth(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # date 지원(주면 dayofweek로 변환)\n",
    "    if \"date\" in X_new.columns:\n",
    "        dt = pd.to_datetime(X_new[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X_new[\"dayofweek\"] = dt.dt.dayofweek\n",
    "        X_new = X_new.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "    X_new = X_new[feature_cols]\n",
    "\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "    return float(pred)\n",
    "\n",
    "\n",
    "def channel_growth_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "tpl = channel_growth_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"channel_growth_model\"))\n",
    "list(tpl.keys())[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_channel_input = {\n",
    "    \"subscriber_count\": 1500000,\n",
    "    \"views_total\": 450000000,\n",
    "    \"video_count_total\": 520,\n",
    "    \"subs_delta_1d\": 1200,\n",
    "    \"views_delta_1d\": 800000,\n",
    "    \"video_count_delta_1d\": 0,\n",
    "    \"subs_delta_7d_mean\": 950,\n",
    "    \"views_delta_7d_mean\": 700000,\n",
    "    \"trending_video_cnt\": 1,\n",
    "    \"trending_views_sum\": 2000000,\n",
    "    \"trending_likes_sum\": 120000,\n",
    "    \"trending_comments_sum\": 9000,\n",
    "    \"dayofweek\": 3,\n",
    "}\n",
    "print(\"pred subscriber growth in 7 days:\", predict_channel_growth(example_channel_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ec26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ✅ 예측 실행 메타 로그 저장 (버전/입력파일 추적)\n",
    "# ==========================================\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "for parent in [PROJECT_ROOT] + list(PROJECT_ROOT.parents):\n",
    "    if (parent / \"data\").exists() and (parent / \"notebooks\").exists():\n",
    "        PROJECT_ROOT = parent\n",
    "        break\n",
    "\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "meta = {\n",
    "    \"timestamp_local\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"inputs\": {\n",
    "        \"trending_daily_path\": str(PATH_TRENDING_DAILY) if \"PATH_TRENDING_DAILY\" in globals() else None,\n",
    "        \"channel_daily_path\": str(PATH_CHANNEL_DAILY) if \"PATH_CHANNEL_DAILY\" in globals() else None,\n",
    "        \"comments_features_path\": str(PATH_COMMENT_VIDEO_FEAT) if \"PATH_COMMENT_VIDEO_FEAT\" in globals() else None,\n",
    "        \"channel_clean_path\": str(PATH_CHANNEL_CLEAN) if \"PATH_CHANNEL_CLEAN\" in globals() else None,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        # 네 13에서 최종 저장하는 파일이 있으면 여기에 추가\n",
    "    }\n",
    "}\n",
    "\n",
    "out_path = REPORTS_DIR / \"predict_run_meta_latest.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ saved:\", out_path)\n",
    "print(json.dumps(meta, ensure_ascii=False, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
