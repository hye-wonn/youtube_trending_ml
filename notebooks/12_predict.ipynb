{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a77a69",
   "metadata": {},
   "source": [
    "ctrl + F → \"기존\" → 코드 원상복귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f87f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e7f1a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root() -> Path:\n",
    "    p = Path.cwd()\n",
    "\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \"data\").exists() and (parent / \"notebooks\").exists():\n",
    "            return parent\n",
    "        \n",
    "    return p\n",
    "\n",
    "def safe_read_csv(path: Path | None):\n",
    "    if path is None:\n",
    "        return None\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(\"⚠️ not found:\", path)\n",
    "        return None\n",
    "    \n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def latest_versioned_csv(folder: Path, base_name: str) -> Path | None:\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+)\\.csv$\")\n",
    "    best_v, best_path = None, None\n",
    "\n",
    "    for f in folder.glob(f\"{base_name}_v*.csv\"):\n",
    "        m = pattern.match(f.name)\n",
    "\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v, best_path = v, f\n",
    "\n",
    "    return best_path\n",
    "\n",
    "def next_versioned_file(folder: Path, base_name: str, ext: str = \".csv\") -> Path:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "    versions = []\n",
    "\n",
    "    for f in folder.glob(f\"{base_name}_v*{ext}\"):\n",
    "        m = pattern.match(f.name)\n",
    "\n",
    "        if m:\n",
    "            versions.append(int(m.group(1)))\n",
    "\n",
    "    v = (max(versions) + 1) if versions else 1\n",
    "\n",
    "    return folder / f\"{base_name}_v{v}{ext}\"\n",
    "\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def run_stamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _safe_str_path(p: Any) -> str | None:\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        return str(Path(p))\n",
    "    except Exception:\n",
    "        return str(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9b7f7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_typed(v: str, default):\n",
    "    \"\"\"default의 타입을 기준으로 입력 문자열을 변환.\"\"\"\n",
    "    v = v.strip()\n",
    "    if v == \"\":\n",
    "        return default\n",
    "\n",
    "    # None default면: 숫자로도, 문자열로도 받을 수 있게\n",
    "    if default is None:\n",
    "        # 숫자 시도\n",
    "        try:\n",
    "            if \".\" in v:\n",
    "                return float(v)\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return v\n",
    "\n",
    "    # bool\n",
    "    if isinstance(default, bool):\n",
    "        vv = v.lower()\n",
    "        if vv in (\"1\", \"true\", \"t\", \"y\", \"yes\"):\n",
    "            return True\n",
    "        if vv in (\"0\", \"false\", \"f\", \"n\", \"no\"):\n",
    "            return False\n",
    "        return default\n",
    "\n",
    "    # int/float\n",
    "    if isinstance(default, (int, float, np.integer, np.floating)):\n",
    "        try:\n",
    "            if isinstance(default, float) or \".\" in v:\n",
    "                return float(v)\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    # 나머지는 문자열로\n",
    "    return v\n",
    "\n",
    "\n",
    "def prompt_dict_input(title: str, template: dict, defaults: dict | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    template: {key: None or default} 형태\n",
    "    defaults: template에 덮어쓸 기본값(optional)\n",
    "    \"\"\"\n",
    "    defaults = defaults or {}\n",
    "    base = {**template, **defaults}  # defaults 우선\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"각 항목에 값을 입력하세요. (Enter: 기본값 사용)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    out = {}\n",
    "    for k, default in base.items():\n",
    "        shown_default = \"\" if default is None else str(default)\n",
    "        v = input(f\"{k} [default={shown_default}]: \")\n",
    "        out[k] = _parse_typed(v, default)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1d3c1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunMetaLogger:\n",
    "    project_root: Path\n",
    "    run_type: str                          # \"predict\" / \"train\" / \"shap\"\n",
    "    run_id: str = field(default_factory=run_stamp)\n",
    "    meta_dir: Path = field(init=False)\n",
    "    meta: dict[str, Any] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.meta_dir = self.project_root / \"reports\" / \"metadata\"\n",
    "        self.meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.meta = {\n",
    "            \"run_type\": self.run_type,\n",
    "            \"run_id\": self.run_id,\n",
    "            \"timestamp_local\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"timestamp_utc\": now_utc_iso(),\n",
    "            \"inputs\": {},\n",
    "            \"outputs\": {},\n",
    "            \"notes\": \"\",\n",
    "        }\n",
    "\n",
    "    def add_input(self, name: str, path: Any):\n",
    "        p = None if path is None else Path(path)\n",
    "        self.meta[\"inputs\"][name] = {\n",
    "            \"path\": _safe_str_path(p),\n",
    "            \"exists\": (p.exists() if isinstance(p, Path) else False) if p is not None else None,\n",
    "        }\n",
    "\n",
    "    def add_inputs_from_globals(self, mapping: dict[str, str], g: dict[str, Any] | None = None):\n",
    "        g = g if g is not None else globals()\n",
    "        for meta_key, var_name in mapping.items():\n",
    "            if var_name in g:\n",
    "                self.add_input(meta_key, g[var_name])\n",
    "            else:\n",
    "                self.add_input(meta_key, None)\n",
    "\n",
    "    def register_output(self, name: str, path: Any, extra: dict[str, Any] | None = None):\n",
    "        p = None if path is None else Path(path)\n",
    "        record: dict[str, Any] = {\"path\": _safe_str_path(p)}\n",
    "        if p is not None:\n",
    "            record[\"exists\"] = p.exists()\n",
    "            if p.exists() and p.is_file():\n",
    "                record[\"size_bytes\"] = p.stat().st_size\n",
    "                record[\"modified_local\"] = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        if extra:\n",
    "            record.update(extra)\n",
    "        self.meta[\"outputs\"][name] = record\n",
    "\n",
    "    def save(self, latest_name: str | None = None) -> tuple[Path, Path | None]:\n",
    "        snapshot_path = self.meta_dir / f\"{self.run_type}_run_meta_{self.run_id}.json\"\n",
    "        with snapshot_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        latest_path = None\n",
    "        if latest_name is None:\n",
    "            latest_name = f\"{self.run_type}_run_meta_latest.json\"\n",
    "        if latest_name:\n",
    "            latest_path = self.meta_dir / latest_name\n",
    "            with latest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return snapshot_path, latest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9865eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\notebooks\n",
      "PROJECT_ROOT: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\n",
      "MODEL_DIR: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\n",
      "PATH_TRENDING_DAILY: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\interim\\01_daily_accumulated\\trending_videos_daily_kr.csv\n",
      "PATH_CHANNEL_DAILY : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\interim\\01_daily_accumulated\\channels_daily_stats_kr.csv\n",
      "PATH_COMMENTS_RAW  : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\raw\\api\\comments_raw_kr.csv\n",
      "PATH_COMMENT_VIDEO_FEATS: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\data\\interim\\02_comment_features\\comment_features_video_level_kr.csv\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = find_project_root()\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n",
    "\n",
    "ACCUM_DIR = PROJECT_ROOT / \"data\" / \"interim\" / \"01_daily_accumulated\"\n",
    "FEAT_DIR  = PROJECT_ROOT / \"data\" / \"interim\" / \"02_comment_features\"\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"api\"\n",
    "\n",
    "PRED_DIR = PROJECT_ROOT / \"reports\" / \"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATH_TRENDING_DAILY = ACCUM_DIR / \"trending_videos_daily_kr.csv\"\n",
    "PATH_CHANNEL_DAILY  = ACCUM_DIR / \"channels_daily_stats_kr.csv\"\n",
    "PATH_COMMENTS_RAW   = RAW_DIR / \"comments_raw_kr.csv\"\n",
    "\n",
    "PATH_COMMENT_VIDEO_FEATS = FEAT_DIR / \"comment_features_video_level_kr.csv\"\n",
    "\n",
    "PATH_CF_VIDEO_ID = PATH_COMMENT_VIDEO_FEATS\n",
    "PATH_CF = None  # 두 번째 merge 스킵\n",
    "\n",
    "print(\"PATH_TRENDING_DAILY:\", PATH_TRENDING_DAILY)\n",
    "print(\"PATH_CHANNEL_DAILY :\", PATH_CHANNEL_DAILY)\n",
    "print(\"PATH_COMMENTS_RAW  :\", PATH_COMMENTS_RAW)\n",
    "print(\"PATH_COMMENT_VIDEO_FEATS:\", PATH_COMMENT_VIDEO_FEATS)\n",
    "\n",
    "# 존재 체크\n",
    "for p in [PATH_TRENDING_DAILY, PATH_CHANNEL_DAILY, PATH_COMMENTS_RAW]:\n",
    "    if not p.exists():\n",
    "        print(\"⚠️ not found:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28e45e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Run Meta Logger 초기화\n",
    "# - train: 모델 학습/저장 메타\n",
    "# - predict: 예측 실행/산출물 메타\n",
    "# =========================\n",
    "\n",
    "train_logger = RunMetaLogger(project_root=PROJECT_ROOT, run_type=\"train\")\n",
    "train_logger.add_inputs_from_globals({\n",
    "    \"trending_daily\": \"PATH_TRENDING_DAILY\",\n",
    "    \"channel_daily\": \"PATH_CHANNEL_DAILY\",\n",
    "    \"comments_video_features\": \"PATH_COMMENT_VIDEO_FEATS\",\n",
    "    \"comments_raw\": \"PATH_COMMENTS_RAW\",\n",
    "})\n",
    "\n",
    "predict_logger = RunMetaLogger(project_root=PROJECT_ROOT, run_type=\"predict\")\n",
    "predict_logger.add_inputs_from_globals({\n",
    "    \"trending_daily\": \"PATH_TRENDING_DAILY\",\n",
    "    \"channel_daily\": \"PATH_CHANNEL_DAILY\",\n",
    "    \"comments_video_features\": \"PATH_COMMENT_VIDEO_FEATS\",\n",
    "    \"comments_raw\": \"PATH_COMMENTS_RAW\",\n",
    "})\n",
    "\n",
    "pred_path = PRED_DIR / f\"pred_{predict_logger.run_id}.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61427f96",
   "metadata": {},
   "source": [
    "## 1) 영상 트렌딩 유지기간 예측 (video_id 단위 회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cab0fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video dataset: (111, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_duration_days</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>region</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>first_trending_date</th>\n",
       "      <th>views_day1</th>\n",
       "      <th>likes_day1</th>\n",
       "      <th>comments_day1</th>\n",
       "      <th>views_mean</th>\n",
       "      <th>views_max</th>\n",
       "      <th>likes_mean</th>\n",
       "      <th>likes_max</th>\n",
       "      <th>comments_mean</th>\n",
       "      <th>comments_max</th>\n",
       "      <th>days_to_first_trending</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>unique_authors</th>\n",
       "      <th>mean_like_count</th>\n",
       "      <th>mean_text_len</th>\n",
       "      <th>url_ratio</th>\n",
       "      <th>hashtag_ratio</th>\n",
       "      <th>mention_ratio</th>\n",
       "      <th>korean_comment_ratio</th>\n",
       "      <th>mean_hangul_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-KrUN6LPxkA</td>\n",
       "      <td>2</td>\n",
       "      <td>UCapCtlV2EcT6obhREqvLbug</td>\n",
       "      <td>KR</td>\n",
       "      <td>10</td>\n",
       "      <td>2026-01-28 09:03:11</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>51584</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>212</td>\n",
       "      <td>57200.0</td>\n",
       "      <td>62816</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>216</td>\n",
       "      <td>3.622789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-WGFbInX6JI</td>\n",
       "      <td>1</td>\n",
       "      <td>UC1q4Ihlv_YhLELw-ijE0Diw</td>\n",
       "      <td>KR</td>\n",
       "      <td>20</td>\n",
       "      <td>2026-01-30 12:30:00</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>94052</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>380</td>\n",
       "      <td>94052.0</td>\n",
       "      <td>94052</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>209.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.004785</td>\n",
       "      <td>34.794258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.665986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0HXwT4gefnQ</td>\n",
       "      <td>1</td>\n",
       "      <td>UCpqyr6h4RCXCEswHlkSjykA</td>\n",
       "      <td>KR</td>\n",
       "      <td>20</td>\n",
       "      <td>2026-01-30 09:00:02</td>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>299453</td>\n",
       "      <td>16687.0</td>\n",
       "      <td>1304</td>\n",
       "      <td>299453.0</td>\n",
       "      <td>299453</td>\n",
       "      <td>16687.0</td>\n",
       "      <td>16687.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>1304</td>\n",
       "      <td>0.624977</td>\n",
       "      <td>216.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>51.361111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245370</td>\n",
       "      <td>0.202387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  trending_duration_days                channel_id region  \\\n",
       "0  -KrUN6LPxkA                       2  UCapCtlV2EcT6obhREqvLbug     KR   \n",
       "1  -WGFbInX6JI                       1  UC1q4Ihlv_YhLELw-ijE0Diw     KR   \n",
       "2  0HXwT4gefnQ                       1  UCpqyr6h4RCXCEswHlkSjykA     KR   \n",
       "\n",
       "   category_id        publish_date first_trending_date  views_day1  \\\n",
       "0           10 2026-01-28 09:03:11          2026-02-01       51584   \n",
       "1           20 2026-01-30 12:30:00          2026-01-31       94052   \n",
       "2           20 2026-01-30 09:00:02          2026-01-31      299453   \n",
       "\n",
       "   likes_day1  comments_day1  views_mean  views_max  likes_mean  likes_max  \\\n",
       "0      1094.0            212     57200.0      62816      1119.0     1144.0   \n",
       "1      2856.0            380     94052.0      94052      2856.0     2856.0   \n",
       "2     16687.0           1304    299453.0     299453     16687.0    16687.0   \n",
       "\n",
       "   comments_mean  comments_max  days_to_first_trending  comment_count  \\\n",
       "0          214.0           216                3.622789            NaN   \n",
       "1          380.0           380                0.479167          209.0   \n",
       "2         1304.0          1304                0.624977          216.0   \n",
       "\n",
       "   unique_authors  mean_like_count  mean_text_len  url_ratio  hashtag_ratio  \\\n",
       "0             NaN              NaN            NaN        NaN            NaN   \n",
       "1           193.0         4.004785      34.794258        0.0        0.00000   \n",
       "2           215.0         0.291667      51.361111        0.0        0.00463   \n",
       "\n",
       "   mention_ratio  korean_comment_ratio  mean_hangul_ratio  \n",
       "0            NaN                   NaN                NaN  \n",
       "1            0.0              0.909091           0.665986  \n",
       "2            0.0              0.245370           0.202387  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_trending_duration_dataset():\n",
    "    trending = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "\n",
    "    # merge 안정성: video_id 타입 통일\n",
    "    trending[\"video_id\"] = trending[\"video_id\"].astype(str)\n",
    "\n",
    "    # 호환: 예전 누적 파일이 collected_date를 썼다면 date로 변환\n",
    "    if \"date\" not in trending.columns and \"collected_date\" in trending.columns:\n",
    "        trending = trending.rename(columns={\"collected_date\": \"date\"})\n",
    "\n",
    "    # 날짜 tz 통일(UTC로 파싱 후 tz 제거 -> naive)\n",
    "    trending[\"date\"] = pd.to_datetime(trending[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    if \"publish_date\" in trending.columns:\n",
    "        trending[\"publish_date\"] = pd.to_datetime(trending[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # target: video_id별 트렌딩 유지기간(일)\n",
    "    y = (trending.groupby(\"video_id\")[\"date\"]\n",
    "                .nunique()\n",
    "                .rename(\"trending_duration_days\")\n",
    "                .reset_index())\n",
    "\n",
    "    # first day snapshot\n",
    "    first_day = (trending.sort_values([\"video_id\", \"date\"])\n",
    "                            .groupby(\"video_id\", as_index=False)\n",
    "                            .first())\n",
    "\n",
    "    cols = [\"video_id\"]\n",
    "    for c in [\"channel_id\", \"region\", \"category_id\", \"publish_date\", \"date\", \"views\", \"likes\", \"comments\"]:\n",
    "        if c in first_day.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    first_day = first_day[cols].rename(columns={\n",
    "        \"views\": \"views_day1\",\n",
    "        \"likes\": \"likes_day1\",\n",
    "        \"comments\": \"comments_day1\",\n",
    "        \"date\": \"first_trending_date\"\n",
    "    })\n",
    "\n",
    "    # aggregates\n",
    "    agg = (trending.groupby(\"video_id\")\n",
    "                    .agg(\n",
    "                        views_mean=(\"views\", \"mean\"),\n",
    "                        views_max=(\"views\", \"max\"),\n",
    "                        likes_mean=(\"likes\", \"mean\"),\n",
    "                        likes_max=(\"likes\", \"max\"),\n",
    "                        comments_mean=(\"comments\", \"mean\"),\n",
    "                        comments_max=(\"comments\", \"max\"),\n",
    "                        trending_days=(\"date\", \"nunique\"),\n",
    "                    )\n",
    "                    .reset_index())\n",
    "\n",
    "    df = y.merge(first_day, on=\"video_id\", how=\"left\").merge(agg, on=\"video_id\", how=\"left\")\n",
    "\n",
    "    # merge 이후 dtype 꼬임 방지: 다시 datetime 강제\n",
    "    if \"publish_date\" in df.columns:\n",
    "        df[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    if \"first_trending_date\" in df.columns:\n",
    "        df[\"first_trending_date\"] = pd.to_datetime(df[\"first_trending_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # days_to_first_trending\n",
    "    df[\"days_to_first_trending\"] = (\n",
    "        (df[\"first_trending_date\"] - df[\"publish_date\"]).dt.total_seconds() / 86400.0\n",
    "        if (\"publish_date\" in df.columns and \"first_trending_date\" in df.columns)\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    # merge comment features (video-level)\n",
    "    vcfv = safe_read_csv(PATH_CF_VIDEO_ID)\n",
    "    if vcfv is not None and \"video_id\" in vcfv.columns:\n",
    "        vcfv = vcfv.copy()\n",
    "        vcfv[\"video_id\"] = vcfv[\"video_id\"].astype(str)\n",
    "        vcfv = vcfv[~vcfv[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcfv = vcfv.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcfv.drop(columns=[\"category_name\"], errors=\"ignore\"), on=\"video_id\", how=\"left\")\n",
    "\n",
    "    vcf = safe_read_csv(PATH_CF)\n",
    "    if vcf is not None and \"video_id\" in vcf.columns:\n",
    "        vcf = vcf.copy()\n",
    "        vcf[\"video_id\"] = vcf[\"video_id\"].astype(str)\n",
    "        vcf = vcf[~vcf[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcf = vcf.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcf, on=\"video_id\", how=\"left\", suffixes=(\"\", \"_vcf\"))\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # leakage 제거\n",
    "    df.drop(columns=[\"trending_days\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # raw/ID/원문 컬럼 제거\n",
    "    DROP_COLS = [\"comment_id\", \"comment_publishedAt\", \"text\", \"run_id\", \"category_name\", \"country\", \"likeCount\"]\n",
    "    df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_video = build_trending_duration_dataset()\n",
    "print(\"video dataset:\", df_video.shape)\n",
    "display(df_video.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ff229a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG MODEL_DIR = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\n",
      "Drop constant cols: ['region'] \n",
      "[Channel Growth] MAE: 0.3650  RMSE: 0.5273  R2: 0.0061\n",
      "Features used: 25\n",
      "DEBUG: about to save\n",
      "DEBUG model_path = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n",
      "DEBUG cols_path  = c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n",
      "saved:\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n",
      "RETURNED: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_feature_columns_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "def train_trending_duration_model(df, test_size=0.2, random_state=42, prefix=\"trending_duration\"):\n",
    "    target_col = \"trending_duration_days\"\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    print(\"DEBUG MODEL_DIR =\", MODEL_DIR)\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    X = df.drop(columns=[\"video_id\", target_col], errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # datetime 컬럼 -> 숫자화(ts/dow)\n",
    "    datetime_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X.columns and c not in datetime_cols:\n",
    "            X[c] = pd.to_datetime(X[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            datetime_cols.append(c)\n",
    "\n",
    "    datetime_cols = list(dict.fromkeys(datetime_cols))\n",
    "    for c in datetime_cols:\n",
    "        dt = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "        ts = dt.astype(\"int64\")\n",
    "        ts = pd.Series(ts, index=X.index).where(dt.notna(), np.nan) / 1e9\n",
    "        X[c + \"_ts\"] = ts\n",
    "        X[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    if datetime_cols:\n",
    "        X = X.drop(columns=datetime_cols, errors=\"ignore\")\n",
    "\n",
    "    # object지만 숫자열이면 numeric 변환\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        print(\"Drop constant cols:\", const_cols[:20], \"...\" if len(const_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    # cat/num 분리\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    # 전처리 파이프라인\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    # ==========================================\n",
    "    # n_samples 작을 때 split 방어\n",
    "    # ==========================================\n",
    "    if len(y) < 10:\n",
    "        print(\"⚠️ n_samples < 10 → holdout split 없이 전체 데이터로 학습(구조 검증 목적)\")\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        # 평가 지표는 의미 없으니 None 처리\n",
    "        mae = rmse = r2 = None\n",
    "\n",
    "        print(\"[Channel Growth] fitted on all data (no test split)\")\n",
    "        print(\"Features used:\", X.shape[1])\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        pred = pipe.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "        r2 = r2_score(y_test, pred)\n",
    "\n",
    "        print(f\"[Channel Growth] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "        print(\"Features used:\", X.shape[1])\n",
    "\n",
    "\n",
    "    # 자동 저장\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "    print(\"DEBUG: about to save\")\n",
    "    print(\"DEBUG model_path =\", model_path)\n",
    "    print(\"DEBUG cols_path  =\", cols_path)\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    assert model_path.exists(), f\"모델 저장 실패: {model_path}\"\n",
    "    assert cols_path.exists(),  f\"컬럼 저장 실패: {cols_path}\"\n",
    "\n",
    "    metrics = {\n",
    "    \"mae\": None if mae is None else float(mae),\n",
    "    \"rmse\": None if rmse is None else float(rmse),\n",
    "    \"r2\": None if r2 is None else float(r2),\n",
    "    \"n_samples\": int(len(y)),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path, metrics\n",
    "\n",
    "video_model, video_feature_cols, video_model_path, video_cols_path, video_metrics = train_trending_duration_model(df_video)\n",
    "print(\"RETURNED:\", video_model_path, video_cols_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2233953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template columns: 25\n",
      "loaded from: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\trending_duration_model_v1.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['channel_id',\n",
       " 'category_id',\n",
       " 'views_day1',\n",
       " 'likes_day1',\n",
       " 'comments_day1',\n",
       " 'views_mean',\n",
       " 'views_max',\n",
       " 'likes_mean',\n",
       " 'likes_max',\n",
       " 'comments_mean',\n",
       " 'comments_max',\n",
       " 'days_to_first_trending',\n",
       " 'comment_count',\n",
       " 'unique_authors',\n",
       " 'mean_like_count',\n",
       " 'mean_text_len',\n",
       " 'url_ratio',\n",
       " 'hashtag_ratio',\n",
       " 'mention_ratio',\n",
       " 'korean_comment_ratio',\n",
       " 'mean_hangul_ratio',\n",
       " 'publish_date_ts',\n",
       " 'publish_date_dow',\n",
       " 'first_trending_date_ts',\n",
       " 'first_trending_date_dow']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _latest_versioned_file(base_dir: str, base_name: str, ext: str = \".joblib\") -> str:\n",
    "    pattern = re.compile(rf\"{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "    best_v = None\n",
    "    best_path = None\n",
    "\n",
    "    for f in os.listdir(base_dir):\n",
    "        m = pattern.match(f)\n",
    "\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v = v\n",
    "                best_path = os.path.join(base_dir, f)\n",
    "\n",
    "    if best_path is None:\n",
    "        raise FileNotFoundError(f\"{base_dir} 에 '{base_name}_v*.joblib' 파일이 없습니다.\")\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "def _load_latest_model_and_cols(model_base_name: str, cols_base_name: str):\n",
    "    model_path = _latest_versioned_file(MODEL_DIR, model_base_name)\n",
    "    cols_path  = _latest_versioned_file(MODEL_DIR, cols_base_name)\n",
    "\n",
    "    pipe = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(cols_path)\n",
    "\n",
    "    return pipe, feature_cols, model_path, cols_path\n",
    "\n",
    "def predict_trending_duration(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # 날짜 입력을 줬다면 학습 때와 동일하게 파생 생성\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X_new.columns:\n",
    "            dt = pd.to_datetime(X_new[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            X_new[c + \"_ts\"] = dt.astype(\"int64\") / 1e9\n",
    "            X_new[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    # 학습 피처 컬럼에 맞추기 (없는 컬럼은 NaN)\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "\n",
    "    X_new = X_new[feature_cols]\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "\n",
    "    return float(pred)\n",
    "\n",
    "def trending_duration_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "tpl = trending_duration_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"trending_duration_model\"))\n",
    "list(tpl.keys())[:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1956a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred trending duration(days): 1.6264927248677252\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Trending Duration 예측 (사용자 입력 or example)\n",
    "# =========================================\n",
    "\n",
    "example_video_input = {\n",
    "    \"category_id\": 24,\n",
    "    \"views_day1\": 120000,\n",
    "    \"likes_day1\": 8000,\n",
    "    \"comments_day1\": 1500,\n",
    "    \"views_mean\": 180000,\n",
    "    \"views_max\": 350000,\n",
    "    \"likes_mean\": 12000,\n",
    "    \"likes_max\": 22000,\n",
    "    \"comments_mean\": 2000,\n",
    "    \"comments_max\": 4200,\n",
    "    \"days_to_first_trending\": 2.0,\n",
    "}\n",
    "\n",
    "use_example = input(\"Video Data (Enter: example 사용 / y: 직접 입력): \").strip().lower()\n",
    "if use_example != \"y\":\n",
    "    video_input = example_video_input\n",
    "else:\n",
    "    # 모델 템플릿(학습 피처) 기반으로 입력 받되, example 값으로 기본값 세팅\n",
    "    tpl = trending_duration_input_template()\n",
    "    # 여기서는 최소 항목만 받도록 subset 구성(원하면 tpl 전체로도 가능)\n",
    "    subset_keys = list(example_video_input.keys())\n",
    "    subset_tpl = {k: None for k in subset_keys}\n",
    "    video_input = prompt_dict_input(\"Trending Duration 입력\", subset_tpl, defaults=example_video_input)\n",
    "\n",
    "pred_td = predict_trending_duration(video_input)\n",
    "print(\"pred trending duration(days):\", pred_td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfede4a",
   "metadata": {},
   "source": [
    "## 2) 채널 성장 예측 (channel_id×date 단위 회귀)\n",
    "타깃: subscriber_growth_h = (h일 뒤 구독자수 - 오늘 구독자수). 기본 h=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19915d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[channel coverage] channels=5, rows=10\n",
      "[channel coverage] horizon_days=1 → need >= 2 days/channel\n",
      "[channel coverage] per-channel days (min/median/max): 1 / 2.0 / 3\n",
      "[channel coverage] eligible channels: 3 / 5\n",
      "[result] channel growth dataset shape: (5, 20)\n",
      "channel dataset: (5, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>run_ts_utc</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>created_date</th>\n",
       "      <th>subscriber_count</th>\n",
       "      <th>views_total</th>\n",
       "      <th>video_count_total</th>\n",
       "      <th>country</th>\n",
       "      <th>subscriber_future</th>\n",
       "      <th>subscriber_growth_h</th>\n",
       "      <th>subs_delta_1d</th>\n",
       "      <th>views_delta_1d</th>\n",
       "      <th>video_count_delta_1d</th>\n",
       "      <th>subs_delta_7d_mean</th>\n",
       "      <th>views_delta_7d_mean</th>\n",
       "      <th>trending_video_cnt</th>\n",
       "      <th>trending_views_sum</th>\n",
       "      <th>trending_likes_sum</th>\n",
       "      <th>trending_comments_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-01-30T19:50:41.252668+00:00</td>\n",
       "      <td>UCMjeoedkGftL8SQh1iO5k9w</td>\n",
       "      <td>Yoo Hwe-seung - Topic</td>\n",
       "      <td>2023-06-26T05:33:12.896775Z</td>\n",
       "      <td>10700</td>\n",
       "      <td>46073857</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21955</td>\n",
       "      <td>582.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>2026-02-01T11:21:08.803708+00:00</td>\n",
       "      <td>UCMjeoedkGftL8SQh1iO5k9w</td>\n",
       "      <td>Yoo Hwe-seung - Topic</td>\n",
       "      <td>2023-06-26T05:33:12.896775Z</td>\n",
       "      <td>10700</td>\n",
       "      <td>46180674</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53408.5</td>\n",
       "      <td>1</td>\n",
       "      <td>45735</td>\n",
       "      <td>772.0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>2026-01-30T19:50:41.252668+00:00</td>\n",
       "      <td>UCrpcd5WtOrdCsx5cufc4JRQ</td>\n",
       "      <td>ZUTOMAYO - Topic</td>\n",
       "      <td>2018-08-29T11:15:36Z</td>\n",
       "      <td>8070</td>\n",
       "      <td>475280828</td>\n",
       "      <td>311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8070.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>133641</td>\n",
       "      <td>4192.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                        run_ts_utc                channel_id  \\\n",
       "0 2026-01-31  2026-01-30T19:50:41.252668+00:00  UCMjeoedkGftL8SQh1iO5k9w   \n",
       "1 2026-02-01  2026-02-01T11:21:08.803708+00:00  UCMjeoedkGftL8SQh1iO5k9w   \n",
       "2 2026-01-31  2026-01-30T19:50:41.252668+00:00  UCrpcd5WtOrdCsx5cufc4JRQ   \n",
       "\n",
       "            channel_name                 created_date  subscriber_count  \\\n",
       "0  Yoo Hwe-seung - Topic  2023-06-26T05:33:12.896775Z             10700   \n",
       "1  Yoo Hwe-seung - Topic  2023-06-26T05:33:12.896775Z             10700   \n",
       "2       ZUTOMAYO - Topic         2018-08-29T11:15:36Z              8070   \n",
       "\n",
       "   views_total  video_count_total  country  subscriber_future  \\\n",
       "0     46073857                 28      NaN            10700.0   \n",
       "1     46180674                 28      NaN            10700.0   \n",
       "2    475280828                311      NaN             8070.0   \n",
       "\n",
       "   subscriber_growth_h  subs_delta_1d  views_delta_1d  video_count_delta_1d  \\\n",
       "0                  0.0            0.0             0.0                   0.0   \n",
       "1                  0.0            0.0        106817.0                   0.0   \n",
       "2                  0.0            0.0             0.0                   0.0   \n",
       "\n",
       "   subs_delta_7d_mean  views_delta_7d_mean  trending_video_cnt  \\\n",
       "0                 0.0                  0.0                   1   \n",
       "1                 0.0              53408.5                   1   \n",
       "2                 0.0                  0.0                   1   \n",
       "\n",
       "   trending_views_sum  trending_likes_sum  trending_comments_sum  \n",
       "0               21955               582.0                     39  \n",
       "1               45735               772.0                     41  \n",
       "2              133641              4192.0                     94  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_channel_growth_dataset(horizon_days=7):\n",
    "    ch = pd.read_csv(PATH_CHANNEL_DAILY)\n",
    "\n",
    "    # 날짜 파싱\n",
    "    ch[\"date\"] = pd.to_datetime(\n",
    "        ch.get(\"date\"), errors=\"coerce\", utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    ch = ch.sort_values([\"channel_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # 숫자형 강제\n",
    "    for c in [\"subscriber_count\", \"views_total\", \"video_count_total\"]:\n",
    "        if c in ch.columns:\n",
    "            ch[c] = pd.to_numeric(ch[c], errors=\"coerce\")\n",
    "\n",
    "    # 채널별 관측 일수 사전 체크\n",
    "    per_channel_days = ch.groupby(\"channel_id\")[\"date\"].nunique()\n",
    "\n",
    "    need_days = horizon_days + 1\n",
    "    eligible_channels = per_channel_days[per_channel_days >= need_days].index\n",
    "\n",
    "    print(\n",
    "        f\"[channel coverage] channels={per_channel_days.shape[0]}, \"\n",
    "        f\"rows={len(ch)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] horizon_days={horizon_days} → \"\n",
    "        f\"need >= {need_days} days/channel\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] per-channel days \"\n",
    "        f\"(min/median/max): \"\n",
    "        f\"{per_channel_days.min()} / \"\n",
    "        f\"{per_channel_days.median()} / \"\n",
    "        f\"{per_channel_days.max()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] eligible channels: \"\n",
    "        f\"{len(eligible_channels)} / {per_channel_days.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    if len(eligible_channels) == 0:\n",
    "        raise ValueError(\n",
    "            f\"No channel has >= {need_days} days of data. \"\n",
    "            f\"Current max days per channel = {per_channel_days.max()}. \"\n",
    "            f\"Collect more daily data or lower horizon_days.\"\n",
    "        )\n",
    "\n",
    "    # 가능한 채널만 유지\n",
    "    ch = ch[ch[\"channel_id\"].isin(eligible_channels)].copy()\n",
    "\n",
    "    # 타깃 생성\n",
    "    ch[\"subscriber_future\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subscriber_count\"]\n",
    "          .shift(-horizon_days)\n",
    "    )\n",
    "    ch[\"subscriber_growth_h\"] = (\n",
    "        ch[\"subscriber_future\"] - ch[\"subscriber_count\"]\n",
    "    )\n",
    "\n",
    "    # 변화량 피처\n",
    "    ch[\"subs_delta_1d\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subscriber_count\"]\n",
    "          .diff(1)\n",
    "          .fillna(0)\n",
    "    )\n",
    "\n",
    "    if \"views_total\" in ch.columns:\n",
    "        ch[\"views_delta_1d\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"views_total\"]\n",
    "              .diff(1)\n",
    "              .fillna(0)\n",
    "        )\n",
    "\n",
    "    if \"video_count_total\" in ch.columns:\n",
    "        ch[\"video_count_delta_1d\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"video_count_total\"]\n",
    "              .diff(1)\n",
    "              .fillna(0)\n",
    "        )\n",
    "\n",
    "    # 7일 평균 추세\n",
    "    ch[\"subs_delta_7d_mean\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subs_delta_1d\"]\n",
    "          .rolling(7, min_periods=1)\n",
    "          .mean()\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    if \"views_delta_1d\" in ch.columns:\n",
    "        ch[\"views_delta_7d_mean\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"views_delta_1d\"]\n",
    "              .rolling(7, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # 트렌딩(채널-일자) merge\n",
    "    tr = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "\n",
    "    tr[\"date\"] = pd.to_datetime(\n",
    "        tr.get(\"date\"), errors=\"coerce\", utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    tr[\"channel_id\"] = tr[\"channel_id\"].astype(str)\n",
    "    tr[\"video_id\"] = tr[\"video_id\"].astype(str)\n",
    "\n",
    "    agg_dict = {\"video_id\": \"nunique\"}\n",
    "    for c in [\"views\", \"likes\", \"comments\"]:\n",
    "        if c in tr.columns:\n",
    "            tr[c] = pd.to_numeric(tr[c], errors=\"coerce\")\n",
    "            agg_dict[c] = \"sum\"\n",
    "\n",
    "    tr_agg = (\n",
    "        tr.groupby([\"channel_id\", \"date\"], as_index=False)\n",
    "          .agg(agg_dict)\n",
    "          .rename(columns={\n",
    "              \"video_id\": \"trending_video_cnt\",\n",
    "              \"views\": \"trending_views_sum\",\n",
    "              \"likes\": \"trending_likes_sum\",\n",
    "              \"comments\": \"trending_comments_sum\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    df = ch.merge(tr_agg, on=[\"channel_id\", \"date\"], how=\"left\")\n",
    "\n",
    "    # 트렌딩 결측 = 0\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"trending_\"):\n",
    "            df[c] = df[c].fillna(0)\n",
    "\n",
    "    # inf 처리\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    df = df[df[\"subscriber_growth_h\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    print(f\"[result] channel growth dataset shape: {df.shape}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================================\n",
    "# Channel Growth dataset 생성 (입력 or 기본값)\n",
    "# =========================================\n",
    "\n",
    "h = input(\"Channel Growth Horizon Days (Enter = 1): \").strip()\n",
    "HORIZON_DAYS = int(h) if h else 1\n",
    "\n",
    "df_channel = build_channel_growth_dataset(horizon_days=HORIZON_DAYS)\n",
    "\n",
    "print(\"channel dataset:\", df_channel.shape)\n",
    "display(df_channel.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "14f81e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop all-NaN cols: ['country'] \n",
      "[Channel Growth] MAE: 0.0000  RMSE: 0.0000  R2: nan\n",
      "Features used: 9\n",
      "saved:\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\channel_growth_model_v1.joblib\n",
      " - c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\channel_growth_feature_columns_v1.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73bib\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " ['subscriber_count',\n",
       "  'views_total',\n",
       "  'video_count_total',\n",
       "  'views_delta_1d',\n",
       "  'views_delta_7d_mean',\n",
       "  'trending_views_sum',\n",
       "  'trending_likes_sum',\n",
       "  'trending_comments_sum',\n",
       "  'dayofweek'],\n",
       " WindowsPath('c:/Users/73bib/Desktop/유혜원/제주한라대학교/[2025] 프로젝트/bigdata_project/youtube_trending_ml/models/channel_growth_model_v1.joblib'),\n",
       " WindowsPath('c:/Users/73bib/Desktop/유혜원/제주한라대학교/[2025] 프로젝트/bigdata_project/youtube_trending_ml/models/channel_growth_feature_columns_v1.joblib'))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_channel_growth_model(df, test_size=0.2, random_state=42, prefix=\"channel_growth\"):\n",
    "    target_col = \"subscriber_growth_h\"\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    X = df.drop(columns=[target_col, \"subscriber_future\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # ID 제거 (식별자 과적합 방지)\n",
    "    X = X.drop(columns=[\"channel_id\"], errors=\"ignore\")\n",
    "\n",
    "    # date -> dayofweek\n",
    "    if \"date\" in X.columns:\n",
    "        X[\"date\"] = pd.to_datetime(X[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X[\"dayofweek\"] = X[\"date\"].dt.dayofweek\n",
    "        X = X.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    # inf 제거\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    DROP_FOR_GROWTH = [\"run_ts_utc\", \"channel_name\", \"created_date\"]\n",
    "    X = X.drop(columns=DROP_FOR_GROWTH, errors=\"ignore\")\n",
    "    \n",
    "    # object인데 숫자열이면 numeric으로 바꾸기\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=900, random_state=random_state, n_jobs=-1, min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"[Channel Growth] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "    print(\"Features used:\", X.shape[1])\n",
    "\n",
    "    # 자동 저장\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    metrics = {\n",
    "    \"mae\": None if mae is None else float(mae),\n",
    "    \"rmse\": None if rmse is None else float(rmse),\n",
    "    \"r2\": None if r2 is None else float(r2),\n",
    "    \"n_samples\": int(len(y)),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path, metrics\n",
    "\n",
    "\n",
    "channel_model, channel_feature_cols, channel_model_path, channel_cols_path, channel_metrics = train_channel_growth_model(df_channel)\n",
    "len(channel_feature_cols), channel_feature_cols[:25], channel_model_path, channel_cols_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f8313592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train meta snapshot: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\reports\\metadata\\train_run_meta_20260202_034137.json\n",
      "✅ train meta latest  : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\reports\\metadata\\train_run_meta_latest.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 학습 메타 로그 저장\n",
    "# ==========================================\n",
    "\n",
    "# outputs: 모델 파일\n",
    "train_logger.register_output(\"trending_duration_model\", video_model_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.register_output(\"trending_duration_feature_columns\", video_cols_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.meta.setdefault(\"metrics\", {})[\"trending_duration\"] = video_metrics\n",
    "\n",
    "train_logger.register_output(\"channel_growth_model\", channel_model_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.register_output(\"channel_growth_feature_columns\", channel_cols_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.meta.setdefault(\"metrics\", {})[\"channel_growth\"] = channel_metrics\n",
    "train_logger.meta.setdefault(\"params\", {})[\"channel_growth_horizon_days\"] = int(HORIZON_DAYS)\n",
    "\n",
    "# snapshot/latest 파일명\n",
    "train_meta_snapshot_path = train_logger.project_root / \"reports\" / \"metadata\" / f\"train_run_meta_{train_logger.run_id}.json\"\n",
    "train_meta_latest_path   = train_logger.project_root / \"reports\" / \"metadata\" / \"train_run_meta_latest.json\"\n",
    "\n",
    "train_snapshot_path, train_latest_path = train_logger.save()\n",
    "\n",
    "train_logger.register_output(\"meta_snapshot\", train_meta_snapshot_path, extra={\"format\": \"json\"})\n",
    "train_logger.register_output(\"meta_latest\", train_meta_latest_path, extra={\"format\": \"json\"})\n",
    "\n",
    "train_logger.save()\n",
    "\n",
    "print(\"✅ train meta snapshot:\", train_snapshot_path)\n",
    "print(\"✅ train meta latest  :\", train_latest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90faca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template columns: 9\n",
      "loaded from: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\models\\channel_growth_model_v1.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['subscriber_count',\n",
       " 'views_total',\n",
       " 'video_count_total',\n",
       " 'views_delta_1d',\n",
       " 'views_delta_7d_mean',\n",
       " 'trending_views_sum',\n",
       " 'trending_likes_sum',\n",
       " 'trending_comments_sum',\n",
       " 'dayofweek']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_channel_growth(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # date 지원(주면 dayofweek로 변환)\n",
    "    if \"date\" in X_new.columns:\n",
    "        dt = pd.to_datetime(X_new[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X_new[\"dayofweek\"] = dt.dt.dayofweek\n",
    "        X_new = X_new.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "\n",
    "    X_new = X_new[feature_cols]\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "\n",
    "    return float(pred)\n",
    "\n",
    "def channel_growth_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "tpl = channel_growth_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"channel_growth_model\"))\n",
    "list(tpl.keys())[:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41edb012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred subscriber growth in 1 days: 0.0\n",
      "✅ saved predictions: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\reports\\predictions\\pred_20260202_034137.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Channel Growth 예측 (사용자 입력 or example)\n",
    "# =========================================\n",
    "\n",
    "# 최신 모델의 \"실제 입력 컬럼\" 템플릿\n",
    "tpl = channel_growth_input_template()\n",
    "\n",
    "# 템플릿 기반 example (tpl 키만 사용)\n",
    "example_channel_input = {\n",
    "    \"subscriber_count\": 1_500_000,\n",
    "    \"views_total\": 450_000_000,\n",
    "    \"video_count_total\": 520,\n",
    "    \"views_delta_1d\": 800_000,\n",
    "    \"views_delta_7d_mean\": 700_000,\n",
    "    \"trending_views_sum\": 2_000_000,\n",
    "    \"trending_likes_sum\": 120_000,\n",
    "    \"trending_comments_sum\": 9_000,\n",
    "    \"dayofweek\": 3,\n",
    "}\n",
    "\n",
    "# tpl에 없는 키 제거 (안전장치)\n",
    "example_channel_input = {k: example_channel_input.get(k, None) for k in tpl.keys()}\n",
    "\n",
    "# -------------------------\n",
    "# 입력 방식 선택\n",
    "# -------------------------\n",
    "use_example = input(\n",
    "    \"Channel Data (Enter: example 사용 / y: 직접 입력): \"\n",
    ").strip().lower()\n",
    "\n",
    "if use_example != \"y\":\n",
    "    ch_input = example_channel_input\n",
    "else:\n",
    "    ch_input = prompt_dict_input(\n",
    "        title=\"Channel Growth 입력\",\n",
    "        template=tpl,\n",
    "        defaults=example_channel_input\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 예측 실행\n",
    "# -------------------------\n",
    "pred_value = predict_channel_growth(ch_input)\n",
    "print(f\"pred subscriber growth in {HORIZON_DAYS} days:\", pred_value)\n",
    "\n",
    "# -------------------------\n",
    "# 예측 결과 저장\n",
    "# -------------------------\n",
    "df_pred = pd.DataFrame([{\n",
    "    **ch_input,\n",
    "    f\"pred_subscriber_growth_{HORIZON_DAYS}d\": pred_value,\n",
    "    \"run_id\": predict_logger.run_id,\n",
    "    \"timestamp_local\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}])\n",
    "\n",
    "df_pred.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ saved predictions:\", pred_path)\n",
    "\n",
    "# outputs 자동 기록\n",
    "predict_logger.register_output(\n",
    "    \"predictions\",\n",
    "    pred_path,\n",
    "    extra={\n",
    "        \"rows\": int(len(df_pred)),\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "predict_logger.meta.setdefault(\"params\", {})[\"channel_growth_horizon_days\"] = int(HORIZON_DAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c5ec26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ meta snapshot: c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\reports\\metadata\\predict_run_meta_20260202_034137.json\n",
      "✅ meta latest  : c:\\Users\\73bib\\Desktop\\유혜원\\제주한라대학교\\[2025] 프로젝트\\bigdata_project\\youtube_trending_ml\\reports\\metadata\\predict_run_meta_latest.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 예측 실행 메타 로그 저장\n",
    "# ==========================================\n",
    "\n",
    "meta_snapshot_path = predict_logger.project_root / \"reports\" / \"metadata\" / f\"predict_run_meta_{predict_logger.run_id}.json\"\n",
    "meta_latest_path   = predict_logger.project_root / \"reports\" / \"metadata\" / \"predict_run_meta_latest.json\"\n",
    "\n",
    "snapshot_path, latest_path = predict_logger.save()\n",
    "\n",
    "predict_logger.register_output(\n",
    "    \"meta_snapshot\",\n",
    "    meta_snapshot_path,\n",
    "    extra={\"format\": \"json\"}\n",
    ")\n",
    "\n",
    "predict_logger.register_output(\n",
    "    \"meta_latest\",\n",
    "    meta_latest_path,\n",
    "    extra={\"format\": \"json\"}\n",
    ")\n",
    "\n",
    "# outputs까지 포함된 최종 메타 저장 (snapshot + latest)\n",
    "predict_logger.save()\n",
    "\n",
    "print(\"✅ meta snapshot:\", snapshot_path)\n",
    "print(\"✅ meta latest  :\", latest_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
