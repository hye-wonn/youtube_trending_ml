{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root() -> Path:\n",
    "    p = Path.cwd()\n",
    "\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \"data\").exists() and (parent / \"notebooks\").exists():\n",
    "            return parent\n",
    "        \n",
    "    return p\n",
    "\n",
    "def safe_read_csv(path: Path | None):\n",
    "    if path is None:\n",
    "        return None\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(\"⚠️ not found:\", path)\n",
    "        return None\n",
    "    \n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def latest_versioned_csv(folder: Path, base_name: str) -> Path | None:\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+)\\.csv$\")\n",
    "    best_v, best_path = None, None\n",
    "\n",
    "    for f in folder.glob(f\"{base_name}_v*.csv\"):\n",
    "        m = pattern.match(f.name)\n",
    "\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v, best_path = v, f\n",
    "\n",
    "    return best_path\n",
    "\n",
    "def next_versioned_file(folder: Path, base_name: str, ext: str = \".csv\") -> Path:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pattern = re.compile(rf\"^{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "    versions = []\n",
    "\n",
    "    for f in folder.glob(f\"{base_name}_v*{ext}\"):\n",
    "        m = pattern.match(f.name)\n",
    "\n",
    "        if m:\n",
    "            versions.append(int(m.group(1)))\n",
    "\n",
    "    v = (max(versions) + 1) if versions else 1\n",
    "\n",
    "    return folder / f\"{base_name}_v{v}{ext}\"\n",
    "\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def run_stamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _safe_str_path(p: Any) -> str | None:\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        return str(Path(p))\n",
    "    except Exception:\n",
    "        return str(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_typed(v: str, default):\n",
    "    \"\"\"default의 타입을 기준으로 입력 문자열을 변환.\"\"\"\n",
    "    v = v.strip()\n",
    "    if v == \"\":\n",
    "        return default\n",
    "\n",
    "    # None default면: 숫자로도, 문자열로도 받을 수 있게\n",
    "    if default is None:\n",
    "        # 숫자 시도\n",
    "        try:\n",
    "            if \".\" in v:\n",
    "                return float(v)\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return v\n",
    "\n",
    "    # bool\n",
    "    if isinstance(default, bool):\n",
    "        vv = v.lower()\n",
    "        if vv in (\"1\", \"true\", \"t\", \"y\", \"yes\"):\n",
    "            return True\n",
    "        if vv in (\"0\", \"false\", \"f\", \"n\", \"no\"):\n",
    "            return False\n",
    "        return default\n",
    "\n",
    "    # int/float\n",
    "    if isinstance(default, (int, float, np.integer, np.floating)):\n",
    "        try:\n",
    "            if isinstance(default, float) or \".\" in v:\n",
    "                return float(v)\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    # 나머지는 문자열로\n",
    "    return v\n",
    "\n",
    "\n",
    "def prompt_dict_input(title: str, template: dict, defaults: dict | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    template: {key: None or default} 형태\n",
    "    defaults: template에 덮어쓸 기본값(optional)\n",
    "    \"\"\"\n",
    "    defaults = defaults or {}\n",
    "    base = {**template, **defaults}  # defaults 우선\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"각 항목에 값을 입력하세요. (Enter: 기본값 사용)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    out = {}\n",
    "    for k, default in base.items():\n",
    "        shown_default = \"\" if default is None else str(default)\n",
    "        v = input(f\"{k} [default={shown_default}]: \")\n",
    "        out[k] = _parse_typed(v, default)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunMetaLogger:\n",
    "    project_root: Path\n",
    "    run_type: str                          # \"predict\" / \"train\" / \"shap\"\n",
    "    run_id: str = field(default_factory=run_stamp)\n",
    "    meta_dir: Path = field(init=False)\n",
    "    meta: dict[str, Any] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.meta_dir = self.project_root / \"reports\" / \"metadata\"\n",
    "        self.meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.meta = {\n",
    "            \"run_type\": self.run_type,\n",
    "            \"run_id\": self.run_id,\n",
    "            \"timestamp_local\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"timestamp_utc\": now_utc_iso(),\n",
    "            \"inputs\": {},\n",
    "            \"outputs\": {},\n",
    "            \"notes\": \"\",\n",
    "        }\n",
    "\n",
    "    def add_input(self, name: str, path: Any):\n",
    "        p = None if path is None else Path(path)\n",
    "        self.meta[\"inputs\"][name] = {\n",
    "            \"path\": _safe_str_path(p),\n",
    "            \"exists\": (p.exists() if isinstance(p, Path) else False) if p is not None else None,\n",
    "        }\n",
    "\n",
    "    def add_inputs_from_globals(self, mapping: dict[str, str], g: dict[str, Any] | None = None):\n",
    "        g = g if g is not None else globals()\n",
    "        for meta_key, var_name in mapping.items():\n",
    "            if var_name in g:\n",
    "                self.add_input(meta_key, g[var_name])\n",
    "            else:\n",
    "                self.add_input(meta_key, None)\n",
    "\n",
    "    def register_output(self, name: str, path: Any, extra: dict[str, Any] | None = None):\n",
    "        p = None if path is None else Path(path)\n",
    "        record: dict[str, Any] = {\"path\": _safe_str_path(p)}\n",
    "        if p is not None:\n",
    "            record[\"exists\"] = p.exists()\n",
    "            if p.exists() and p.is_file():\n",
    "                record[\"size_bytes\"] = p.stat().st_size\n",
    "                record[\"modified_local\"] = datetime.fromtimestamp(p.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        if extra:\n",
    "            record.update(extra)\n",
    "        self.meta[\"outputs\"][name] = record\n",
    "\n",
    "    def save(self, latest_name: str | None = None) -> tuple[Path, Path | None]:\n",
    "        snapshot_path = self.meta_dir / f\"{self.run_type}_run_meta_{self.run_id}.json\"\n",
    "        with snapshot_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        latest_path = None\n",
    "        if latest_name is None:\n",
    "            latest_name = f\"{self.run_type}_run_meta_latest.json\"\n",
    "        if latest_name:\n",
    "            latest_path = self.meta_dir / latest_name\n",
    "            with latest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return snapshot_path, latest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = find_project_root()\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n",
    "\n",
    "ACCUM_DIR = PROJECT_ROOT / \"data\" / \"interim\" / \"01_daily_accumulated\"\n",
    "FEAT_DIR  = PROJECT_ROOT / \"data\" / \"interim\" / \"02_comment_features\"\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"api\"\n",
    "\n",
    "PRED_DIR = PROJECT_ROOT / \"reports\" / \"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATH_TRENDING_DAILY = ACCUM_DIR / \"trending_videos_daily_kr.csv\"\n",
    "PATH_CHANNEL_DAILY  = ACCUM_DIR / \"channels_daily_stats_kr.csv\"\n",
    "PATH_COMMENTS_RAW   = RAW_DIR / \"comments_raw_kr.csv\"\n",
    "\n",
    "PATH_COMMENT_VIDEO_FEATS = FEAT_DIR / \"comment_features_video_level_kr.csv\"\n",
    "\n",
    "PATH_CF_VIDEO_ID = PATH_COMMENT_VIDEO_FEATS\n",
    "PATH_CF = None  # 두 번째 merge 스킵\n",
    "\n",
    "print(\"PATH_TRENDING_DAILY:\", PATH_TRENDING_DAILY)\n",
    "print(\"PATH_CHANNEL_DAILY :\", PATH_CHANNEL_DAILY)\n",
    "print(\"PATH_COMMENTS_RAW  :\", PATH_COMMENTS_RAW)\n",
    "print(\"PATH_COMMENT_VIDEO_FEATS:\", PATH_COMMENT_VIDEO_FEATS)\n",
    "\n",
    "# 존재 체크\n",
    "for p in [PATH_TRENDING_DAILY, PATH_CHANNEL_DAILY, PATH_COMMENTS_RAW]:\n",
    "    if not p.exists():\n",
    "        print(\"⚠️ not found:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e45e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Run Meta Logger 초기화\n",
    "# - train: 모델 학습/저장 메타\n",
    "# - predict: 예측 실행/산출물 메타\n",
    "# =========================\n",
    "\n",
    "train_logger = RunMetaLogger(project_root=PROJECT_ROOT, run_type=\"train\")\n",
    "train_logger.add_inputs_from_globals({\n",
    "    \"trending_daily\": \"PATH_TRENDING_DAILY\",\n",
    "    \"channel_daily\": \"PATH_CHANNEL_DAILY\",\n",
    "    \"comments_video_features\": \"PATH_COMMENT_VIDEO_FEATS\",\n",
    "    \"comments_raw\": \"PATH_COMMENTS_RAW\",\n",
    "})\n",
    "\n",
    "predict_logger = RunMetaLogger(project_root=PROJECT_ROOT, run_type=\"predict\")\n",
    "predict_logger.add_inputs_from_globals({\n",
    "    \"trending_daily\": \"PATH_TRENDING_DAILY\",\n",
    "    \"channel_daily\": \"PATH_CHANNEL_DAILY\",\n",
    "    \"comments_video_features\": \"PATH_COMMENT_VIDEO_FEATS\",\n",
    "    \"comments_raw\": \"PATH_COMMENTS_RAW\",\n",
    "})\n",
    "\n",
    "pred_path = PRED_DIR / f\"pred_{predict_logger.run_id}.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61427f96",
   "metadata": {},
   "source": [
    "## 1) 영상 트렌딩 유지기간 예측 (video_id 단위 회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trending_duration_dataset():\n",
    "    trending = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "\n",
    "    # merge 안정성: video_id 타입 통일\n",
    "    trending[\"video_id\"] = trending[\"video_id\"].astype(str)\n",
    "\n",
    "    # 호환: 예전 누적 파일이 collected_date를 썼다면 date로 변환\n",
    "    if \"date\" not in trending.columns and \"collected_date\" in trending.columns:\n",
    "        trending = trending.rename(columns={\"collected_date\": \"date\"})\n",
    "\n",
    "    # 날짜 tz 통일(UTC로 파싱 후 tz 제거 -> naive)\n",
    "    trending[\"date\"] = pd.to_datetime(trending[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    if \"publish_date\" in trending.columns:\n",
    "        trending[\"publish_date\"] = pd.to_datetime(trending[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # target: video_id별 트렌딩 유지기간(일)\n",
    "    y = (trending.groupby(\"video_id\")[\"date\"]\n",
    "                .nunique()\n",
    "                .rename(\"trending_duration_days\")\n",
    "                .reset_index())\n",
    "\n",
    "    # first day snapshot\n",
    "    first_day = (trending.sort_values([\"video_id\", \"date\"])\n",
    "                            .groupby(\"video_id\", as_index=False)\n",
    "                            .first())\n",
    "\n",
    "    cols = [\"video_id\"]\n",
    "    for c in [\"channel_id\", \"region\", \"category_id\", \"publish_date\", \"date\", \"views\", \"likes\", \"comments\"]:\n",
    "        if c in first_day.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    first_day = first_day[cols].rename(columns={\n",
    "        \"views\": \"views_day1\",\n",
    "        \"likes\": \"likes_day1\",\n",
    "        \"comments\": \"comments_day1\",\n",
    "        \"date\": \"first_trending_date\"\n",
    "    })\n",
    "\n",
    "    # aggregates\n",
    "    agg = (trending.groupby(\"video_id\")\n",
    "                    .agg(\n",
    "                        views_mean=(\"views\", \"mean\"),\n",
    "                        views_max=(\"views\", \"max\"),\n",
    "                        likes_mean=(\"likes\", \"mean\"),\n",
    "                        likes_max=(\"likes\", \"max\"),\n",
    "                        comments_mean=(\"comments\", \"mean\"),\n",
    "                        comments_max=(\"comments\", \"max\"),\n",
    "                        trending_days=(\"date\", \"nunique\"),\n",
    "                    )\n",
    "                    .reset_index())\n",
    "\n",
    "    df = y.merge(first_day, on=\"video_id\", how=\"left\").merge(agg, on=\"video_id\", how=\"left\")\n",
    "\n",
    "    # merge 이후 dtype 꼬임 방지: 다시 datetime 강제\n",
    "    if \"publish_date\" in df.columns:\n",
    "        df[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    if \"first_trending_date\" in df.columns:\n",
    "        df[\"first_trending_date\"] = pd.to_datetime(df[\"first_trending_date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # days_to_first_trending\n",
    "    df[\"days_to_first_trending\"] = (\n",
    "        (df[\"first_trending_date\"] - df[\"publish_date\"]).dt.total_seconds() / 86400.0\n",
    "        if (\"publish_date\" in df.columns and \"first_trending_date\" in df.columns)\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    # merge comment features (video-level)\n",
    "    vcfv = safe_read_csv(PATH_CF_VIDEO_ID)\n",
    "    if vcfv is not None and \"video_id\" in vcfv.columns:\n",
    "        vcfv = vcfv.copy()\n",
    "        vcfv[\"video_id\"] = vcfv[\"video_id\"].astype(str)\n",
    "        vcfv = vcfv[~vcfv[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcfv = vcfv.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcfv.drop(columns=[\"category_name\"], errors=\"ignore\"), on=\"video_id\", how=\"left\")\n",
    "\n",
    "    vcf = safe_read_csv(PATH_CF)\n",
    "    if vcf is not None and \"video_id\" in vcf.columns:\n",
    "        vcf = vcf.copy()\n",
    "        vcf[\"video_id\"] = vcf[\"video_id\"].astype(str)\n",
    "        vcf = vcf[~vcf[\"video_id\"].str.contains(\"#NAME\", na=False)]\n",
    "        vcf = vcf.drop_duplicates(\"video_id\", keep=\"first\")\n",
    "        df = df.merge(vcf, on=\"video_id\", how=\"left\", suffixes=(\"\", \"_vcf\"))\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # leakage 제거\n",
    "    df.drop(columns=[\"trending_days\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # raw/ID/원문 컬럼 제거\n",
    "    DROP_COLS = [\"comment_id\", \"comment_publishedAt\", \"text\", \"run_id\", \"category_name\", \"country\", \"likeCount\"]\n",
    "    df.drop(columns=[c for c in DROP_COLS if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_video = build_trending_duration_dataset()\n",
    "print(\"video dataset:\", df_video.shape)\n",
    "display(df_video.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff229a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trending_duration_model(df, test_size=0.2, random_state=42, prefix=\"trending_duration\"):\n",
    "    target_col = \"trending_duration_days\"\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    print(\"DEBUG MODEL_DIR =\", MODEL_DIR)\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    X = df.drop(columns=[\"video_id\", target_col], errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    # datetime 컬럼 -> 숫자화(ts/dow)\n",
    "    datetime_cols = list(X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns)\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X.columns and c not in datetime_cols:\n",
    "            X[c] = pd.to_datetime(X[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            datetime_cols.append(c)\n",
    "\n",
    "    datetime_cols = list(dict.fromkeys(datetime_cols))\n",
    "    for c in datetime_cols:\n",
    "        dt = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "        ts = dt.astype(\"int64\")\n",
    "        ts = pd.Series(ts, index=X.index).where(dt.notna(), np.nan) / 1e9\n",
    "        X[c + \"_ts\"] = ts\n",
    "        X[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    if datetime_cols:\n",
    "        X = X.drop(columns=datetime_cols, errors=\"ignore\")\n",
    "\n",
    "    # object지만 숫자열이면 numeric 변환\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        print(\"Drop constant cols:\", const_cols[:20], \"...\" if len(const_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    # cat/num 분리\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    # 전처리 파이프라인\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    # ==========================================\n",
    "    # n_samples 작을 때 split 방어\n",
    "    # ==========================================\n",
    "    if len(y) < 10:\n",
    "        print(\"⚠️ n_samples < 10 → holdout split 없이 전체 데이터로 학습(구조 검증 목적)\")\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        # 평가 지표는 의미 없으니 None 처리\n",
    "        mae = rmse = r2 = None\n",
    "\n",
    "        print(\"[Channel Growth] fitted on all data (no test split)\")\n",
    "        print(\"Features used:\", X.shape[1])\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        pred = pipe.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "        r2 = r2_score(y_test, pred)\n",
    "\n",
    "        print(f\"[Channel Growth] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "        print(\"Features used:\", X.shape[1])\n",
    "\n",
    "\n",
    "    # 자동 저장\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "    print(\"DEBUG: about to save\")\n",
    "    print(\"DEBUG model_path =\", model_path)\n",
    "    print(\"DEBUG cols_path  =\", cols_path)\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    assert model_path.exists(), f\"모델 저장 실패: {model_path}\"\n",
    "    assert cols_path.exists(),  f\"컬럼 저장 실패: {cols_path}\"\n",
    "\n",
    "    metrics = {\n",
    "    \"mae\": None if mae is None else float(mae),\n",
    "    \"rmse\": None if rmse is None else float(rmse),\n",
    "    \"r2\": None if r2 is None else float(r2),\n",
    "    \"n_samples\": int(len(y)),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path, metrics\n",
    "\n",
    "video_model, video_feature_cols, video_model_path, video_cols_path, video_metrics = train_trending_duration_model(df_video)\n",
    "print(\"RETURNED:\", video_model_path, video_cols_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latest_versioned_file(base_dir: str, base_name: str, ext: str = \".joblib\") -> str:\n",
    "    pattern = re.compile(rf\"{re.escape(base_name)}_v(\\d+){re.escape(ext)}$\")\n",
    "    best_v = None\n",
    "    best_path = None\n",
    "\n",
    "    for f in os.listdir(base_dir):\n",
    "        m = pattern.match(f)\n",
    "\n",
    "        if m:\n",
    "            v = int(m.group(1))\n",
    "\n",
    "            if best_v is None or v > best_v:\n",
    "                best_v = v\n",
    "                best_path = os.path.join(base_dir, f)\n",
    "\n",
    "    if best_path is None:\n",
    "        raise FileNotFoundError(f\"{base_dir} 에 '{base_name}_v*.joblib' 파일이 없습니다.\")\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "def _load_latest_model_and_cols(model_base_name: str, cols_base_name: str):\n",
    "    model_path = _latest_versioned_file(MODEL_DIR, model_base_name)\n",
    "    cols_path  = _latest_versioned_file(MODEL_DIR, cols_base_name)\n",
    "\n",
    "    pipe = joblib.load(model_path)\n",
    "    feature_cols = joblib.load(cols_path)\n",
    "\n",
    "    return pipe, feature_cols, model_path, cols_path\n",
    "\n",
    "def predict_trending_duration(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # 날짜 입력을 줬다면 학습 때와 동일하게 파생 생성\n",
    "    for c in [\"publish_date\", \"first_trending_date\"]:\n",
    "        if c in X_new.columns:\n",
    "            dt = pd.to_datetime(X_new[c], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "            X_new[c + \"_ts\"] = dt.astype(\"int64\") / 1e9\n",
    "            X_new[c + \"_dow\"] = dt.dt.dayofweek\n",
    "\n",
    "    # 학습 피처 컬럼에 맞추기 (없는 컬럼은 NaN)\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "\n",
    "    X_new = X_new[feature_cols]\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "\n",
    "    return float(pred)\n",
    "\n",
    "def trending_duration_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"trending_duration_model\",\n",
    "        \"trending_duration_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "tpl = trending_duration_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"trending_duration_model\"))\n",
    "list(tpl.keys())[:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Trending Duration 예측 (사용자 입력 or example)\n",
    "# =========================================\n",
    "\n",
    "example_video_input = {\n",
    "    \"category_id\": 24,\n",
    "    \"views_day1\": 120000,\n",
    "    \"likes_day1\": 8000,\n",
    "    \"comments_day1\": 1500,\n",
    "    \"views_mean\": 180000,\n",
    "    \"views_max\": 350000,\n",
    "    \"likes_mean\": 12000,\n",
    "    \"likes_max\": 22000,\n",
    "    \"comments_mean\": 2000,\n",
    "    \"comments_max\": 4200,\n",
    "    \"days_to_first_trending\": 2.0,\n",
    "}\n",
    "\n",
    "use_example = input(\"Video Data (Enter: example 사용 / y: 직접 입력): \").strip().lower()\n",
    "if use_example != \"y\":\n",
    "    video_input = example_video_input\n",
    "else:\n",
    "    # 모델 템플릿(학습 피처) 기반으로 입력 받되, example 값으로 기본값 세팅\n",
    "    tpl = trending_duration_input_template()\n",
    "    # 여기서는 최소 항목만 받도록 subset 구성(원하면 tpl 전체로도 가능)\n",
    "    subset_keys = list(example_video_input.keys())\n",
    "    subset_tpl = {k: None for k in subset_keys}\n",
    "    video_input = prompt_dict_input(\"Trending Duration 입력\", subset_tpl, defaults=example_video_input)\n",
    "\n",
    "pred_td = predict_trending_duration(video_input)\n",
    "print(\"pred trending duration(days):\", pred_td)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfede4a",
   "metadata": {},
   "source": [
    "## 2) 채널 성장 예측 (channel_id×date 단위 회귀)\n",
    "타깃: subscriber_growth_h = (h일 뒤 구독자수 - 오늘 구독자수). 기본 h=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19915d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_channel_growth_dataset(horizon_days=7):\n",
    "    ch = pd.read_csv(PATH_CHANNEL_DAILY)\n",
    "\n",
    "    # 날짜 파싱\n",
    "    ch[\"date\"] = pd.to_datetime(\n",
    "        ch.get(\"date\"), errors=\"coerce\", utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    ch = ch.sort_values([\"channel_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # 숫자형 강제\n",
    "    for c in [\"subscriber_count\", \"views_total\", \"video_count_total\"]:\n",
    "        if c in ch.columns:\n",
    "            ch[c] = pd.to_numeric(ch[c], errors=\"coerce\")\n",
    "\n",
    "    # 채널별 관측 일수 사전 체크\n",
    "    per_channel_days = ch.groupby(\"channel_id\")[\"date\"].nunique()\n",
    "\n",
    "    need_days = horizon_days + 1\n",
    "    eligible_channels = per_channel_days[per_channel_days >= need_days].index\n",
    "\n",
    "    print(\n",
    "        f\"[channel coverage] channels={per_channel_days.shape[0]}, \"\n",
    "        f\"rows={len(ch)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] horizon_days={horizon_days} → \"\n",
    "        f\"need >= {need_days} days/channel\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] per-channel days \"\n",
    "        f\"(min/median/max): \"\n",
    "        f\"{per_channel_days.min()} / \"\n",
    "        f\"{per_channel_days.median()} / \"\n",
    "        f\"{per_channel_days.max()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[channel coverage] eligible channels: \"\n",
    "        f\"{len(eligible_channels)} / {per_channel_days.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    if len(eligible_channels) == 0:\n",
    "        raise ValueError(\n",
    "            f\"No channel has >= {need_days} days of data. \"\n",
    "            f\"Current max days per channel = {per_channel_days.max()}. \"\n",
    "            f\"Collect more daily data or lower horizon_days.\"\n",
    "        )\n",
    "\n",
    "    # 가능한 채널만 유지\n",
    "    ch = ch[ch[\"channel_id\"].isin(eligible_channels)].copy()\n",
    "\n",
    "    # 타깃 생성\n",
    "    ch[\"subscriber_future\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subscriber_count\"]\n",
    "          .shift(-horizon_days)\n",
    "    )\n",
    "    ch[\"subscriber_growth_h\"] = (\n",
    "        ch[\"subscriber_future\"] - ch[\"subscriber_count\"]\n",
    "    )\n",
    "\n",
    "    # 변화량 피처\n",
    "    ch[\"subs_delta_1d\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subscriber_count\"]\n",
    "          .diff(1)\n",
    "          .fillna(0)\n",
    "    )\n",
    "\n",
    "    if \"views_total\" in ch.columns:\n",
    "        ch[\"views_delta_1d\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"views_total\"]\n",
    "              .diff(1)\n",
    "              .fillna(0)\n",
    "        )\n",
    "\n",
    "    if \"video_count_total\" in ch.columns:\n",
    "        ch[\"video_count_delta_1d\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"video_count_total\"]\n",
    "              .diff(1)\n",
    "              .fillna(0)\n",
    "        )\n",
    "\n",
    "    # 7일 평균 추세\n",
    "    ch[\"subs_delta_7d_mean\"] = (\n",
    "        ch.groupby(\"channel_id\")[\"subs_delta_1d\"]\n",
    "          .rolling(7, min_periods=1)\n",
    "          .mean()\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    if \"views_delta_1d\" in ch.columns:\n",
    "        ch[\"views_delta_7d_mean\"] = (\n",
    "            ch.groupby(\"channel_id\")[\"views_delta_1d\"]\n",
    "              .rolling(7, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # 트렌딩(채널-일자) merge\n",
    "    tr = pd.read_csv(PATH_TRENDING_DAILY)\n",
    "\n",
    "    tr[\"date\"] = pd.to_datetime(\n",
    "        tr.get(\"date\"), errors=\"coerce\", utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    tr[\"channel_id\"] = tr[\"channel_id\"].astype(str)\n",
    "    tr[\"video_id\"] = tr[\"video_id\"].astype(str)\n",
    "\n",
    "    agg_dict = {\"video_id\": \"nunique\"}\n",
    "    for c in [\"views\", \"likes\", \"comments\"]:\n",
    "        if c in tr.columns:\n",
    "            tr[c] = pd.to_numeric(tr[c], errors=\"coerce\")\n",
    "            agg_dict[c] = \"sum\"\n",
    "\n",
    "    tr_agg = (\n",
    "        tr.groupby([\"channel_id\", \"date\"], as_index=False)\n",
    "          .agg(agg_dict)\n",
    "          .rename(columns={\n",
    "              \"video_id\": \"trending_video_cnt\",\n",
    "              \"views\": \"trending_views_sum\",\n",
    "              \"likes\": \"trending_likes_sum\",\n",
    "              \"comments\": \"trending_comments_sum\",\n",
    "          })\n",
    "    )\n",
    "\n",
    "    df = ch.merge(tr_agg, on=[\"channel_id\", \"date\"], how=\"left\")\n",
    "\n",
    "    # 트렌딩 결측 = 0\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"trending_\"):\n",
    "            df[c] = df[c].fillna(0)\n",
    "\n",
    "    # inf 처리\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    df = df[df[\"subscriber_growth_h\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    print(f\"[result] channel growth dataset shape: {df.shape}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================================\n",
    "# Channel Growth dataset 생성 (입력 or 기본값)\n",
    "# =========================================\n",
    "\n",
    "h = input(\"Channel Growth Horizon Days (Enter = 1): \").strip()\n",
    "HORIZON_DAYS = int(h) if h else 1\n",
    "\n",
    "df_channel = build_channel_growth_dataset(horizon_days=HORIZON_DAYS)\n",
    "\n",
    "print(\"channel dataset:\", df_channel.shape)\n",
    "display(df_channel.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f81e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_channel_growth_model(df, test_size=0.2, random_state=42, prefix=\"channel_growth\"):\n",
    "    target_col = \"subscriber_growth_h\"\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"'{target_col}' 컬럼이 df에 없습니다.\")\n",
    "\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    X = df.drop(columns=[target_col, \"subscriber_future\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # ID 제거 (식별자 과적합 방지)\n",
    "    X = X.drop(columns=[\"channel_id\"], errors=\"ignore\")\n",
    "\n",
    "    # date -> dayofweek\n",
    "    if \"date\" in X.columns:\n",
    "        X[\"date\"] = pd.to_datetime(X[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X[\"dayofweek\"] = X[\"date\"].dt.dayofweek\n",
    "        X = X.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    # inf 제거\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 타깃 NaN 제거\n",
    "    valid = y.notna()\n",
    "    X = X.loc[valid].reset_index(drop=True)\n",
    "    y = y.loc[valid].reset_index(drop=True)\n",
    "\n",
    "    DROP_FOR_GROWTH = [\"run_ts_utc\", \"channel_name\", \"created_date\"]\n",
    "    X = X.drop(columns=DROP_FOR_GROWTH, errors=\"ignore\")\n",
    "    \n",
    "    # object인데 숫자열이면 numeric으로 바꾸기\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"object\":\n",
    "            tmp = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "            if tmp.notna().mean() >= 0.9:\n",
    "                X[c] = tmp\n",
    "\n",
    "    # 전부 NaN 컬럼 제거\n",
    "    all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(\"Drop all-NaN cols:\", all_nan_cols[:20], \"...\" if len(all_nan_cols) > 20 else \"\")\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 상수 컬럼 제거\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=900, random_state=random_state, n_jobs=-1, min_samples_leaf=2\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"preprocess\", pre), (\"model\", model)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred) ** 0.5\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"[Channel Growth] MAE: {mae:.4f}  RMSE: {rmse:.4f}  R2: {r2:.4f}\")\n",
    "    print(\"Features used:\", X.shape[1])\n",
    "\n",
    "    # 자동 저장\n",
    "    model_path = next_versioned_file(MODEL_DIR, f\"{prefix}_model\", ext=\".joblib\")\n",
    "    cols_path  = next_versioned_file(MODEL_DIR, f\"{prefix}_feature_columns\", ext=\".joblib\")\n",
    "\n",
    "    joblib.dump(pipe, model_path)\n",
    "    joblib.dump(list(X.columns), cols_path)\n",
    "\n",
    "    metrics = {\n",
    "    \"mae\": None if mae is None else float(mae),\n",
    "    \"rmse\": None if rmse is None else float(rmse),\n",
    "    \"r2\": None if r2 is None else float(r2),\n",
    "    \"n_samples\": int(len(y)),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "\n",
    "    print(\"saved:\")\n",
    "    print(\" -\", model_path)\n",
    "    print(\" -\", cols_path)\n",
    "\n",
    "    return pipe, list(X.columns), model_path, cols_path, metrics\n",
    "\n",
    "\n",
    "channel_model, channel_feature_cols, channel_model_path, channel_cols_path, channel_metrics = train_channel_growth_model(df_channel)\n",
    "len(channel_feature_cols), channel_feature_cols[:25], channel_model_path, channel_cols_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8313592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 학습 메타 로그 저장\n",
    "# ==========================================\n",
    "\n",
    "# outputs: 모델 파일\n",
    "train_logger.register_output(\"trending_duration_model\", video_model_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.register_output(\"trending_duration_feature_columns\", video_cols_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.meta.setdefault(\"metrics\", {})[\"trending_duration\"] = video_metrics\n",
    "\n",
    "train_logger.register_output(\"channel_growth_model\", channel_model_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.register_output(\"channel_growth_feature_columns\", channel_cols_path, extra={\"format\": \"joblib\"})\n",
    "train_logger.meta.setdefault(\"metrics\", {})[\"channel_growth\"] = channel_metrics\n",
    "train_logger.meta.setdefault(\"params\", {})[\"channel_growth_horizon_days\"] = int(HORIZON_DAYS)\n",
    "\n",
    "# snapshot/latest 파일명\n",
    "train_meta_snapshot_path = train_logger.project_root / \"reports\" / \"metadata\" / f\"train_run_meta_{train_logger.run_id}.json\"\n",
    "train_meta_latest_path   = train_logger.project_root / \"reports\" / \"metadata\" / \"train_run_meta_latest.json\"\n",
    "\n",
    "train_snapshot_path, train_latest_path = train_logger.save()\n",
    "\n",
    "train_logger.register_output(\"meta_snapshot\", train_meta_snapshot_path, extra={\"format\": \"json\"})\n",
    "train_logger.register_output(\"meta_latest\", train_meta_latest_path, extra={\"format\": \"json\"})\n",
    "\n",
    "train_logger.save()\n",
    "\n",
    "print(\"✅ train meta snapshot:\", train_snapshot_path)\n",
    "print(\"✅ train meta latest  :\", train_latest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90faca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_channel_growth(input_dict: dict) -> float:\n",
    "    pipe, feature_cols, model_path, cols_path = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "    X_new = pd.DataFrame([input_dict]).copy()\n",
    "\n",
    "    # date 지원(주면 dayofweek로 변환)\n",
    "    if \"date\" in X_new.columns:\n",
    "        dt = pd.to_datetime(X_new[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        X_new[\"dayofweek\"] = dt.dt.dayofweek\n",
    "        X_new = X_new.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "    for c in feature_cols:\n",
    "        if c not in X_new.columns:\n",
    "            X_new[c] = np.nan\n",
    "\n",
    "    X_new = X_new[feature_cols]\n",
    "    pred = pipe.predict(X_new)[0]\n",
    "\n",
    "    return float(pred)\n",
    "\n",
    "def channel_growth_input_template():\n",
    "    _, cols, _, _ = _load_latest_model_and_cols(\n",
    "        \"channel_growth_model\",\n",
    "        \"channel_growth_feature_columns\",\n",
    "    )\n",
    "    return {c: None for c in cols}\n",
    "\n",
    "tpl = channel_growth_input_template()\n",
    "print(\"template columns:\", len(tpl))\n",
    "print(\"loaded from:\", _latest_versioned_file(MODEL_DIR, \"channel_growth_model\"))\n",
    "list(tpl.keys())[:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Channel Growth 예측 (사용자 입력 or example)\n",
    "# =========================================\n",
    "\n",
    "# 최신 모델의 \"실제 입력 컬럼\" 템플릿\n",
    "tpl = channel_growth_input_template()\n",
    "\n",
    "# 템플릿 기반 example (tpl 키만 사용)\n",
    "example_channel_input = {\n",
    "    \"subscriber_count\": 1_500_000,\n",
    "    \"views_total\": 450_000_000,\n",
    "    \"video_count_total\": 520,\n",
    "    \"views_delta_1d\": 800_000,\n",
    "    \"views_delta_7d_mean\": 700_000,\n",
    "    \"trending_views_sum\": 2_000_000,\n",
    "    \"trending_likes_sum\": 120_000,\n",
    "    \"trending_comments_sum\": 9_000,\n",
    "    \"dayofweek\": 3,\n",
    "}\n",
    "\n",
    "# tpl에 없는 키 제거 (안전장치)\n",
    "example_channel_input = {k: example_channel_input.get(k, None) for k in tpl.keys()}\n",
    "\n",
    "# -------------------------\n",
    "# 입력 방식 선택\n",
    "# -------------------------\n",
    "use_example = input(\n",
    "    \"Channel Data (Enter: example 사용 / y: 직접 입력): \"\n",
    ").strip().lower()\n",
    "\n",
    "if use_example != \"y\":\n",
    "    ch_input = example_channel_input\n",
    "else:\n",
    "    ch_input = prompt_dict_input(\n",
    "        title=\"Channel Growth 입력\",\n",
    "        template=tpl,\n",
    "        defaults=example_channel_input\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# 예측 실행\n",
    "# -------------------------\n",
    "pred_value = predict_channel_growth(ch_input)\n",
    "print(f\"pred subscriber growth in {HORIZON_DAYS} days:\", pred_value)\n",
    "\n",
    "# -------------------------\n",
    "# 예측 결과 저장\n",
    "# -------------------------\n",
    "df_pred = pd.DataFrame([{\n",
    "    **ch_input,\n",
    "    f\"pred_subscriber_growth_{HORIZON_DAYS}d\": pred_value,\n",
    "    \"run_id\": predict_logger.run_id,\n",
    "    \"timestamp_local\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}])\n",
    "\n",
    "df_pred.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ saved predictions:\", pred_path)\n",
    "\n",
    "# outputs 자동 기록\n",
    "predict_logger.register_output(\n",
    "    \"predictions\",\n",
    "    pred_path,\n",
    "    extra={\n",
    "        \"rows\": int(len(df_pred)),\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "predict_logger.meta.setdefault(\"params\", {})[\"channel_growth_horizon_days\"] = int(HORIZON_DAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ec26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 예측 실행 메타 로그 저장\n",
    "# ==========================================\n",
    "\n",
    "meta_snapshot_path = predict_logger.project_root / \"reports\" / \"metadata\" / f\"predict_run_meta_{predict_logger.run_id}.json\"\n",
    "meta_latest_path   = predict_logger.project_root / \"reports\" / \"metadata\" / \"predict_run_meta_latest.json\"\n",
    "\n",
    "snapshot_path, latest_path = predict_logger.save()\n",
    "\n",
    "predict_logger.register_output(\n",
    "    \"meta_snapshot\",\n",
    "    meta_snapshot_path,\n",
    "    extra={\"format\": \"json\"}\n",
    ")\n",
    "\n",
    "predict_logger.register_output(\n",
    "    \"meta_latest\",\n",
    "    meta_latest_path,\n",
    "    extra={\"format\": \"json\"}\n",
    ")\n",
    "\n",
    "# outputs까지 포함된 최종 메타 저장 (snapshot + latest)\n",
    "predict_logger.save()\n",
    "\n",
    "print(\"✅ meta snapshot:\", snapshot_path)\n",
    "print(\"✅ meta latest  :\", latest_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
